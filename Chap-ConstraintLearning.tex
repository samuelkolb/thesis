%!TEX root=Thesis.tex
\chapter{Problem Statement}
\label{cha:problem_statement}

% Concepts
% (Herbrand) Interpretation
% Clause
% Range restricted

%[TODO Write] Given examples we want to learn logical %constraints that describe the data and can be used by a %logical solver to generate models.

%+ We want to model user preferences to be able to find an optimal solution

This thesis attempts to solve two problems: learning constraints from examples and learning user preferences to discriminate between different solutions.
The clause learning process is independent, while clausal discovery requires, amongst others, a set of soft constraints.
Typically, these are produced by the clause learning system.

\section{Learning constraints}

\begin{framed}
	\noindent
	\begin{minipage}{\textwidth}
		\paragraph{Goal 1}
		Given a language of clauses~\sym{L}, a set of examples~\sym{D} and a threshold~$t$, the goal of the constraint learner is to find a complete set $\sym{T} \in 2^\sym{L}$ of maximally specific clauses, that each cover at least $t$ examples of \sym{D}.
	\end{minipage}
\end{framed}

\paragraph{Language}
The language \sym{L} determines what clauses are considered for inclusion in \sym{T}.
It captures the syntactic restrictions and the bias.
In this case, elements of \sym{L} are syntactically restricted to functor-free (definition~\ref{def:funtor_free}) clauses.
The bias of a system captures the implicit or explicit constraints of the system.
In this case, only range-restricted and connected clauses are considered.
Furthermore, the user explicitly determines the maximal number of variables and terms per clause, which determines the search space.

\paragraph{Examples}
Formally, examples can be seen as Herbrand interpretations, every example consisting of a set of ground atoms.
A clause $c$ is said to cover an interpretation \sym{I} if it is a model of the clause: \cen{$\sym{I} \models \obj{c}$.}
It can often be useful to include domain\,/\,background knowledge.
This knowledge is an input to the learning system and has the form of a logical theory \sym{K}.
If background knowledge is provided, an interpretation \sym{I} is covered, if: \cen{$\sym{I} \models \obj{c} \land \sym{K}$.}
Additionally, users can decide to provide partial interpretations as examples, which are extended to Herbrand interpretations using the background theory.

\paragraph{Threshold}
The threshold $t$ can be used to search for either hard constraints ($t = |\sym{D}|$) or soft constraints, which only have to hold on a subset of \sym{D} ($t < |\sym{D}|$).
This can be useful when constraints might not always hold, for example, when there is noise on the examples.
Logical constraint solvers such as IDP can generate interpretations that adhere to the theory.

\paragraph{Learned theory}
A learned theory \sym{T} contains only variables and is independent of the specific examples that were used.
These constraints can, therefore, be used for other problem domains.
For example, constraints learned from a $4 \times 4$ sudoku can, in principle, be used for a $9 \times 9$ sudoku.
This an advantage of using first-order logical constraints over propositional constraints.

% \subsection{Evaluating constraints}
% Given the output of the learning phase, this thesis will examine the question: can a theory \sym{T} of learned clauses be used successfully to model constraint problems?

% Systems like IDP have been shown [Ref IDP-solving?] to be able to generate models and extend partial models given a logical theory.
% The success of learning constraints from examples will depend on the quality of the theory generated by the learning system.

\section{Learning user preferences}

\begin{framed}
	\noindent
	\begin{minipage}{\textwidth}
		\paragraph{Goal 2}
		Given a set of soft constraints~\sym{S} and a set of rankings~\sym{P} over an example set~\sym{D}, the goal of the clausal optimization process is to assign weights to every clause in~\sym{S} such that the order induced by the weighted clauses~(\sym{W}) optimally approximate the rankings in \sym{P}.
	\end{minipage}
\end{framed}

\paragraph{Soft constraints}
The soft constraints are obtained by running clause learning on \sym{D} with a low threshold.
Theoretically, they could also be imported from another source.

\paragraph{Rankings}
The rankings~\sym{P} are partial orderings over examples from~\sym{D}.
Rankings are of the form: \cen{$G_1 > G_2 > ... > G_n$} and every group~$G_i \subset \sym{D}$ is a set of equally ranked examples. Equalities are not used directly by the clausal optimization algorithm, therefore, every ranking corresponds with a set of pairwise (strict) inequalities.

\paragraph{Weighted clauses}
Every weighted clause consists of a tuple $(w, c) \in \mathbb{R} \times \sym{S}$.
Weights are real numbers, positive or negative.
The larger the weight connected with a clause, the more of a desirable property that clause forms.
Negatively weighted clauses represent unwanted properties, which should be avoided.

Given a set~\sym{W} a score $\mathit{score}(\sym{I})$ can be calculated for each interpretation $\sym{I} \in \sym{D}$.
\begin{align*}
	\mathit{score}(\sym{I}) &= \sum\limits_{(w, c)}^\sym{W} w * v(c, \sym{I}) \\
	v(c, \sym{I}) &=
	\begin{cases}
		1,& \text{if } \sym{I} \models c\\
		0,& \text{otherwise}
	\end{cases}
\end{align*}

The $\mathit{score}$ function induces a total ordering over all examples.
Therefore, the weighted clauses \sym{W} can be used as optimization criteria, making it possible to identify the best solution.

[!! Possible: expand on solving]