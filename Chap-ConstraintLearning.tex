%!TEX root=Thesis.tex
\chapter{Problem Statement}
\label{cha:problem_statement}

% Concepts
% (Herbrand) Interpretation
% Clause
% Range restricted

%[TODO Write] Given examples we want to learn logical %constraints that describe the data and can be used by a %logical solver to generate models.

%+ We want to model user preferences to be able to find an optimal solution

This thesis attempts to solve mainly two problems: learning constraints from examples and learning optimization criteria from user preferences.
The systems that perform these tasks should be easy to use and take into account what kind of information a user can provide.
Finally, the thesis also outlines how these constraints and optimiztion criteria can be used in practice.

\section{Learning constraints}

The first goal of this thesis is to learn constraints that describe a set of positive examples provided by a user.
Such constraints form a formal representation of a problem.
This formal representation can be used by a constraint solver to generate solutions to the problem.

\label{sec:learning_constraints}

\begin{framed}
	\noindent
	\begin{minipage}{\textwidth}
		\paragraph{Goal 1}
		Given a language of clauses~\sym{L}, a set of examples~\sym{D} and a threshold~$t$, the goal of the constraint learner is to find a complete set $\sym{T} \in 2^\sym{L}$ of maximally specific clauses, that each cover at least $t$ examples in \sym{D}.
	\end{minipage}
\end{framed}

\paragraph{Language}
The language \sym{L} determines what clauses are considered for inclusion in~\sym{T}.
In this thesis, \sym{L} is the language of connected, functor-free and range-restricted clauses (definitions in capter~\ref{cha:bg}) and clauses adhere to object identity.
Object identity specifies that variables with different names cannot denote the same object.
Furthermore, the user explicitly determines the maximal number of variables and atoms per clause, which determines the search space.

\paragraph{Examples}
Formally, examples can be seen as Herbrand interpretations, every example consisting of a set of ground atoms.
A Herbrand interpretation is complete, which means that any facts not included in the interpretation are false.
A clause $c$ is said to cover an interpretation \sym{I} if it is a model of the clause: \cen{$\sym{I} \models \obj{c}$.}
It can often be useful to include domain\,/\,background knowledge.
This knowledge is an input to the learning system and has the form of a logical theory \sym{K}.
If background knowledge is provided, an interpretation \sym{I} is covered, if: \cen{$\sym{I} \models \obj{c} \land \sym{K}$.}
Additionally, users can decide to provide partial interpretations as examples, which are extended to Herbrand interpretations using the background theory.

\paragraph{Threshold}
The threshold $t$ can be used to search for either hard constraints ($t = |\sym{D}|$) or soft constraints, which only have to hold on a subset of \sym{D} ($t < |\sym{D}|$).
This can be useful when constraints might not always hold, for example, when there is noise on the examples.

\paragraph{Learned theory}
A learned theory \sym{T} contains only variables and is independent of the specific examples that were used.
These constraints can, therefore, be used for other problem domains.
For example, constraints learned from a $4 \times 4$ sudoku can, in principle, be used for a $9 \times 9$ sudoku.\footnote{An $n \times n$ sudoku consists of $n$ rows, columns and blocks containing each the numbers $1$ to $n$.}
This an advantage of using first-order logical constraints over propositional constraints.

% \subsection{Evaluating constraints}
% Given the output of the learning phase, this thesis will examine the question: can a theory \sym{T} of learned clauses be used successfully to model constraint problems?

% Systems like IDP have been shown [Ref IDP-solving?] to be able to generate models and extend partial models given a logical theory.
% The success of learning constraints from examples will depend on the quality of the theory generated by the learning system.

\section{Learning optimization criteria}
\label{sec:learning_user_preferences}
\label{sec:learning_optimization_criteria}

The second goal of this thesis is to learn optimization criteria that model the users preferences over a set of examples.
These optimization criteria can then be used to distinguish between better and worse solutions and to identify a optimal solution.

\begin{framed}
	\noindent
	\begin{minipage}{\textwidth}
		\paragraph{Goal 2}
		Given a set of example~\sym{D} and a set of rankings~\sym{P} over these examples, the goal of the clausal optimization process is to find soft constraints~\sym{S} and assign weights to them, such that the order induced by these weighted constraints \sym{W} optimally approximate the rankings in \sym{P}
		% Given a set of soft constraints~\sym{S} and a set of rankings~\sym{P} over an example set~\sym{D}, the goal of the clausal optimization process is to assign weights to every clause in~\sym{S} such that the order induced by the weighted clauses~(\sym{W}) optimally approximate the rankings in \sym{P}.
	\end{minipage}
\end{framed}

\paragraph{Rankings}
The rankings~\sym{P} are partial orderings over examples from~\sym{D}.
Rankings are of the form: \cen{$G_1 > G_2 > ... > G_n$} and every group~$G_i \subset \sym{D}$ is a set of equally ranked examples. Equalities are not used directly by the clausal optimization algorithm, therefore, every ranking corresponds with a set of pairwise (strict) inequalities.

\paragraph{Soft constraints}
The soft constraints are clauses that cover some of the examples in~\sym{D}.

\paragraph{Weighted constraints}
Every weighted constraints consists of a tuple $(w, c) \in \mathbb{R} \times \sym{S}$.
Weights are rational numbers, positive or negative.
The larger the weight of a constraints, the more of a desirable property that constraints forms.
Negatively weighted constraints represent unwanted properties, which should be avoided.

Given a set~\sym{W}, a score $\mathit{score}(\sym{I})$ can be calculated for each interpretation $\sym{I} \in \sym{D}$.
\begin{align*}
	\mathit{score}(\sym{I}) &= \sum\limits_{(w, c)}^\sym{W} w * v(c, \sym{I}) \\
	v(c, \sym{I}) &=
	\begin{cases}
		1,& \text{if } \sym{I} \models c\\
		0,& \text{otherwise}
	\end{cases}
\end{align*}

The $\mathit{score}$ function induces a total ordering over all examples.
Therefore, the weighted constraints \sym{W} can be used as optimization criteria by searching for the interpretation that maximizes the score.

\section{Solving}
Hard constraints in the form of clauses can be used directly by a logical solver to generate solutions.
This is not possible for the optimization criteria that are learned by the clausal optimization process.
To the knowledge of the author, there are no solvers that can use the chosen representation directly.
Therefore, it is a third goal of this thesis to outline how these optimization criteria can be used in practice.