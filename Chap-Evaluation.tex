%!TEX root=Thesis.tex
\chapter{Evaluation}
\label{cha:evaluation}

Several experiments have been run in order to assess how well the implementation accomplishes the clause learning and clausal optimization tasks.
Moreover, the influence of different factors internal and external to the algorithm is evaluated.
The clause learning and clausal optimization workflows will be examined separately, in that order.

\section{Learning}

This section attempts to evaluate how well the clause learning system accomplishes the first goal of this thesis.
There are different aspects to be considered.
Firstly, the accuracy of the learned clauses will be examined, as well as the influence of several parameters on the accuracy.
This gives a measure of how well a learned theory approximates the underlying model.
Secondly, it will be evaluated how efficiently this task is accomplished.
Since clausal discovery can take a long time to find a theory on larger examples, it will be important to understand the influence of several factors on the running speed.
In the last part, the differences between learned clauses and constraints given by a user will be examined.

\subsection{Setup}
There are two key problems that are used to evaluate different questions.
Their standard setup will be described in this section.
Different experiments can vary on the specific examples and parameters that are used and it will be mentioned when this is the case.

\paragraph{Map coloring}
The map coloring problem has already been used as a running example in the previous section.
It also serves as a good, albeit small, example for clause learning in practice.
The standard setting uses two examples, each with three countries.
The first example (figure~\ref{fig:setup_mapcolor_benelux}) features two colors, while the second example (figure~\ref{fig:setup_mapcolor_benede}) needs three colors as all countries are connected.

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[height=3.5cm]{MapColoringBenelux.pdf}
  \caption{Map coloring example 1}
  \label{fig:setup_mapcolor_benelux}
\end{minipage}
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[height=3.5cm]{MapColoringBenede.pdf}
  \caption{Map coloring example 2}
  \label{fig:setup_mapcolor_benede}
\end{minipage}
\end{figure}

Map coloring uses types $\mathit{Country}$ and $\mathit{Color}$.
It uses two predicates $\mathit{color/2}$ and $\mathit{neighbor/2}$, the neighbor predicate is symmetric.
Unless mentioned otherwise, the standard parameters for map coloring are $3$ variables and~$3$ terms per clause.

\paragraph{Sudoku}
Sudoku is a slightly more difficult problem.
An $n \times n$ sudoku contains $n$ rows, $n$ columns and $n$ blocks, each of which contains the numbers $1$ to $n$ exactly once.
Every cell is represented by an object of type $\mathit{Cell}$.
The other types describe attributes of cells: $\mathit{Row}$, $\mathit{Colum}$, $\mathit{Block}$ and a numeric type $\mathit{Value}$.
Each of those attributes is related to a cell through the predicates $\mathit{row/2}$, $\mathit{col/2}$, $\mathit{block/2}$ and $\mathit{value/2}$ respectively.
In the standard setting one $4 \times 4$ example (figure~\ref{fig:setup_sudoku}) is provided.

\begin{figure}[!htp]
	\centering
	\begin{tabular}{|cc|cc|}
		\hline
		1 & 3 & 2 & 4 \\
		4 & 2 & 3 & 1 \\ \hline
		2 & 1 & 4 & 3 \\
		3 & 4 & 1 & 2 \\ \hline
	\end{tabular}
	\label{fig:setup_sudoku}
	\caption{$4 \times 4$ sudoku example}
\end{figure}

Clause learning on the sudoku is usually done with $4$ variables and~$4$ terms per clause.
In the example, the row, column, block and value are explicitly given for each cell.
The use of specific, disjoint types instead of one common (numeric) type for rows, columns and blocks makes the clause learning process more efficient.
It prevents the use of the same variable for, for example, a row and a column.

\paragraph{Elevator}
"ELEVATOR"

\paragraph{Co-housing}
The co-housing problem is a somewhat more complex problem.
Two friends (persons) are looking at different situations (areas) to live and work in.
\\\\
\begin{minipage}{0.5\textwidth}
	\begin{verbatim*}
		type Area
		type Person
		type Cost > nat

		calc cheap(Cost)
	\end{verbatim*}
\end{minipage}
\begin{minipage}{0.5\textwidth}
	\begin{verbatim*}
		pred live(Person, Area)
		pred work(Person, Area)
		pred car(Person)

		pred cost(Area, Cost)
	\end{verbatim*}
\end{minipage}

Background knowledge is used to calculate what is considered as cheap.
It also limits the amount of people to $2$ and expresses that every area has only one cost.
There are $5$~examples that each adhere to $4$~constraints:

\begin{enumerate}
	\item If the two persons do not live in the same area, they work in the same area.
	\item If a person does not work and live in the same area, that person needs a car.
	\item A person who lives in a cheap area has a car
	\item If the two persons live together, they live in an expensive area.
\end{enumerate}

\subsection{Accuracy}

\begin{question}
	Do learned theories model the real constraints of a problem?
\end{question}

\begin{experiment}
	\label{exp:cd_acc_map_constraints}
	The standard map coloring problem produces the following constraints:
	\begin{shiftedflalign*}
		\mathit{false} &\leftarrow \mathit{neighbor}(x, x)& \\
		\mathit{false} &\leftarrow \mathit{color}(x, color1) \land \mathit{color}(x, color2)& \\
		\mathit{false} &\leftarrow \mathit{color}(x, color1) \land \mathit{neighbor}(x, y)  \land \mathit{color}(y, color1)&
	\end{shiftedflalign*}
	Recall that variables with different names denote different objects.
	Thus, for example, in the second clause, $\mathit{color1} \neq \mathit{color2}$.

	If the number of variables and terms per clause is increased to $4$, additional constraints are learned such as:
	\begin{shiftedflalign*}
		\mathit{false} &\leftarrow \mathit{color}(x, color1) \land \mathit{color}(y, color1)  \land \mathit{color}(z, color1)&
	\end{shiftedflalign*}
\end{experiment}

\begin{experiment}
	The correctness of the learned theories can be measured by using the logical solver to test the theories on unseen examples.
	Theories are inaccurate if they contain too little constraints or too many constraints.
	In the first case the theory will falsely cover negative examples (false positive), while in the second case it will not cover some positive examples (false negative).
	For the map coloring problem, the three constraints shown in experiment~\ref{exp:cd_acc_map_constraints} can be verified and for the sudoku problem, solved $9 \times 9$ sudokus can serve as positive examples and negative examples when manipulated (switch two numbers such that the resulting sudoku is wrong).

	"ELEVATOR ANNNNNNND COHOUSING"
	=> Find constraints

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{XXX}
			\textbf{Name}	& \textbf{False positives}	& \textbf{False negatives} \\
			\toprule
			Map coloring 	& No				& No \\
			Sudoku 			& No				& No
		\end{tabularx}
		\label{tbl:accuracy_standard}
		\caption{Accuracy standard examples}
	\end{table}

	The two problems produce correct theories for the standard parameters (number of clauses and terms).
	As demonstrated in experiment~\ref{exp:cd_acc_map_constraints}, if these are increased, overfitting can occur.
	The influence of these parameters is discussed in question~\ref{q:cd_acc_influence}.
\end{experiment}

\paragraph{Discussion}
"Discuss"


\begin{question}
	What is the influence of the parameters on the accuracy?
	\label{q:cd_acc_influence}
\end{question}

\begin{experiment}
	\label{exp:cd_acc_influence_par}
	The influence of the number of variables and terms is illustrated on the sudoku and map coloring problem.

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{lcc|XX}
			\textbf{Name} & \textbf{Variables} & \textbf{Terms}	& \textbf{False positives} & \textbf{False negatives} \\
			\toprule
			Map coloring & 2 & 3 & x &   \\
			& 3 & 3 &   &   \\
			& 4 & 3 &   & x \\
			\midrule
			Sudoku & 3 & 3 & x	&   \\
			& 4 & 4 &   	&   \\
			& 6 & 4 &   	&   \\
			& 4 & 6 &   	&   \\
			& 5 & 5 &   	&   \\
			& 6 & 6 &   	& x
		\end{tabularx}
		\label{tbl:cd_acc_influence}
		\caption{Influence of number of variables and terms}
	\end{table}

\end{experiment}

\paragraph{Discussion}
	Overfitting can be avoided by adding more or larger examples.
	For the sudoku problem, adding one $9 \times 9$ example solves overfitting with $6$ variables and $6$ terms.
"Discuss"
Influence range restriction?


\subsection{Speed}

Unless explicitly stated, all problems are run $8$ consecutive times in order to eliminate external influences.
The experiments are run on the same machine\footnote{Mac OSX, 2.3 GHz Intel Core i7 with 4 cores and hyper threading, 16GB RAM}.

\begin{question}
	How fast is the clause learning system?
\end{question}

\begin{experiment}
	In order to gain an overview over the running times and provide a benchmark for tests on various influences, the standard examples are each run using the standard clause learning process.
	
	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{XXX}
			\textbf{Name}	& \textbf{Mean (s)}	& \textbf{StdDev (s)} \\
			\toprule
			Map coloring 	& 1.581				& 0.117 \\
			Sudoku 			& 4.787				& 0.062 \\
			Elevator 		& 3.182 			& 0.073 \\
			Co-housing 		& 25.903			& 0.446
		\end{tabularx}
		\label{tbl:exp_speed_standard}
		\caption{Running times for standard examples}
	\end{table}

\end{experiment}

\paragraph{Discussion}
"Discuss"

\begin{question}
	What is the influence of the parameters and input data on the running time?
\end{question}

\begin{experiment}[\textsc{Parameters}]
	Experiment~\ref{exp:cd_acc_influence_par} showed the influence of the parameters on the accuracy of the algorithm.
	In this experiment the running time for these settings is calculated.

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{lcc|XX}
			\textbf{Name} & \textbf{Variables} & \textbf{Terms}	& \textbf{Mean (s)} & \textbf{StdDev (s)} \\
			\toprule
			Map coloring 	& 2 & 3 & 0.690		& 0.023	\\
							& 3 & 3 & 1.511		& 0.053	\\
							& 4 & 3 & 2.223		& 0.072	\\
			\midrule	
			Sudoku 			& 3 & 3 & 1.258		& 0.023	\\
							& 4 & 4 & 4.768		& 0.058	\\
							& 6 & 4 & 6.269		& 0.050	\\
							& 4 & 6 & 4.746		& 0.047	\\
							& 5 & 5 & 25.995	& 0.168	\\
							& 6 & 6 & 49.163	& 0.267
		\end{tabularx}
		\label{tbl:cd_speed_influence}
		\caption{Influence of number of variables and terms}
	\end{table}

\end{experiment}

\begin{experiment}
	Adding more examples takes a toll on the efficiency of the algorithm.
	This experiment measures the increase of the running time that occurs when examples are added.
	The sudoku problem is run in two settings: $4$ variables and terms and $6$ variables and terms.
	Each setting has three rounds, in each round a $9 \times 9$ solved sudoku example is added to the standard example.

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{c|XX|XX}
			\textbf{$9 \times 9$ sudokus} & \textbf{Terms}	& \textbf{Mean (s)} & \textbf{StdDev (s)} \\
			\toprule
			0 & 4.768 & 0.058	& 49.163	& 0.267	\\
			1 & 5.862 & 0.060	& 63.801	& 0.769	\\
			2 & 7.026 & 0.112	& 80.489	& 0.287	\\
			3 & 7.967 & 0.087	& 99.640	& 0.488	\\
		\end{tabularx}
		\label{tbl:cd_speed_examples}
		\caption{Influence of number of variables and terms}
	\end{table}
\end{experiment}

\paragraph{Discussion}
"Discuss"

\begin{question}
	How do the syntactic restrictions influence the running time?
\end{question}

\begin{experiment}[\textsc{Range restriction}]
	The clause learning algorithm normally only explores range restricted clauses.
	While this measure positively influences the efficiency of the algorithm, it trades off expressiveness.
	In this experiment the standard problems are run, while allowing lifting the constraint that clauses must be range restricted.

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{XXX}
			\textbf{Name}	& \textbf{Mean (s)}	& \textbf{StdDev (s)} \\
			\toprule
			Map coloring 	& 4.629				& 0.199 \\
			Sudoku 			& 16.118			& 0.154 \\
			Elevator 		& 40.453 			& 0.319 \\
			Co-housing 		& 207.768			& 0.330
		\end{tabularx}
		\label{tbl:exp_speed_no_range}
		\caption{Running times for standard examples without range restriction}
	\end{table}

\end{experiment}

\begin{experiment}[\textsc{Connected clauses}]
	"Write"

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{XXX}
			\textbf{Name}	& \textbf{Mean (s)}	& \textbf{StdDev (s)} \\
			\toprule
			Map coloring 	& 1.589				& 0.110 \\
			Sudoku 			& 7.068				& 0.150 \\
			Elevator 		& 6.157 			& 0.114 \\
			Co-housing 		& 103.633			& 0.131
		\end{tabularx}
		\label{tbl:exp_speed_no_connect}
		\caption{Running times for standard examples without connectivity}
	\end{table}

\end{experiment}

\paragraph{Discussion}
"Discuss"

\begin{question}
	What is the influence of the efficiency measures on the running time?
\end{question}

There are mainly two measures incorporated in the clause learning algorithm that aim to reduce the running time.
Contrary to syntactic restrictions these do, in principle, not affect the outcome of the algorithm.

\begin{experiment}[\textsc{Symmetric predicates}]
	The first design choice evaluated is the option to provide symmetric predicates.
	The map coloring example contains a symmetric predicate $\mathit{neighbor}$.
	By replacing this symmetric predicate with a standard predicate, the influence on the running time can be measured.

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{XXX}
			\textbf{Name}	& \textbf{Mean (s)}	& \textbf{StdDev (s)} \\
			\toprule
			Standard & 1.581 & 0.117 \\
			Non-symmetric & 2.541 & 0.128 \\
		\end{tabularx}
		\label{tbl:exp_speed_symm}
		\caption{Running times symmetric vs. non-symmetric map coloring}
	\end{table}

	By running the non-symmetric version $8$ times, it can be compared to the standard map coloring problem.
	In the non-symmetric version an additional constraint is learned to express the symmetry of the neighbor relation: \cen{$\mathit{neighbor}(c_2, c_1) \leftarrow \mathit{neighbor}(c_1, c_2)$.}
\end{experiment}

\begin{experiment}[\textsc{Subset test}]
	As mentioned in chapter~\ref{cha:meth}, a subset test is used to forestall some entailment tests.
	In this experiment, the running times without subset test is measured to evaluate the effect of this test.
	To this end, the standard problems are run without the subset test.

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{XXX}
			\textbf{Name}	& \textbf{Mean (s)}	& \textbf{StdDev (s)} \\
			\toprule
			Map coloring 	& 1.558				& 0.117 \\
			Sudoku 			& 5.239				& 0.140 \\
			Elevator 		& 3.678 			& 0.094 \\
			Co-housing 		& 30.145			& 0.159
		\end{tabularx}
		\label{tbl:exp_speed_no_subset}
		\caption{Running times for standard examples without subset test}
	\end{table}
\end{experiment}

\begin{experiment}[\textsc{Representative test}]
	The same constraint can often be represented by multiple clauses.
	In order to avoid generating different clauses but equivalent, the refinement operator defines an order of clauses.
	If a clause cannot be rewritten as a different clause that occurs earlier in that order, the clause is called representative.
	This measure leads to a reduction of generated clauses.
	In this experiment the standard problems are run without this test.

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{XXX}
			\textbf{Name}	& \textbf{Mean (s)}	& \textbf{StdDev (s)} \\
			\toprule
			Map coloring 	& 2.389				& 0.130 \\
			Sudoku 			& 8.995				& 0.125 \\
			Elevator 		& 3.559 			& 0.076 \\
			Co-housing 		& 55.402			& 0.298
		\end{tabularx}
		\label{tbl:exp_speed_no_representative}
		\caption{Running times for standard examples without representative test}
	\end{table}

\end{experiment}

\paragraph{Discussion}
"Discuss"

\subsection{Compared to human}

This section will briefly examine how learned clauses compare to human programmed theories.

\begin{question}
	What is the relation between machine learned and user provided clauses?
\end{question}

\begin{experiment}
\label{exp:cd_user_show}
	The map coloring problem is simple but can be used to compare the machine learned to human learned constraints.
	For the map coloring problem a theory is provided on the website of the IDP system.
	This examples uses similar types and predicates, with the exception of using a \emph{function} instead of a predicate for the color.
	Types and predicates have been renamed in order to correspond with the names used in the standard example.  

	\begin{shiftedflalign*}
		color(x) \neq color(y) \leftarrow neighbor(x,y) \lor neighbor(y,x).
	\end{shiftedflalign*}

\end{experiment}


\begin{experiment}[\textsc{Speed}]
	This experiment measures the running time of a learned theory versus a human programmed theory.
	"Rewrite"
	The example used is map coloring, and a human implementation for this problem has been obtained from the IDP examples site [!! "footnote"].
	The human made theory uses a function to assign colors instead of a predicate, aside from this the types and predicates are the same as in the standard map coloring example.
	There are two changes that have to be made for generated theory to work on the same examples as the human theory.
	Since the solver is now used to generate solutions, an extra clause has to be added to assure that every color is assigned a color:
	\cen{$\forall \mathit{country}\  \exists \mathit{color} : \mathit{color}(\mathit{country}, \mathit{color}).$}
	Additionally, since the clause learning problem makes use of a symmetric predicate, $\mathit{neighbor}(x, y)$ is replaced by $\mathit{neighbor}(x, y) \lor \mathit{neighbor(y,x)}$.

	The example used is a Leighton graph with $450$ nodes and $9803$ edges\footnote{\url{http://map.gsia.cmu.edu/COLOR/instances/le450_5c.col}}.
	This graph is converted into an IDP structure and the CPU time\footnote{Measured by LUAs inbuilt $os.clock()$} of running model-expansion to find the colors for both the human and learned theory is measured.

	"Sudoku explanation"

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{lr|XX}
			\textbf{Name} & \textbf{Theory} & \textbf{Mean (s)} & \textbf{StdDev (s)} \\
			\toprule
			Map coloring & Human & $0.968$ & $0.023$ \\
			& Learned & $0.403$ & $0.015$ \\
			\midrule
			Sudoku & Human & $1.453$ & $0.018$ \\ 
			& Learned & $0.156$ & $0.008$ \\
			& Corrected & $0.310$ & $0.012$
		\end{tabularx}
		\label{tbl:speed_human_machine}
		\caption{Running times human vs.~learned theory}
	\end{table}

\end{experiment}

\paragraph{Discussion}
"Discuss"

 - P Damaging vs. errors

 - P No redundant clauses (user modelling knows what he wants)


 - P More expressiveness

 Expert users can make better use of the logical solver to solve problems faster.
 However, running time is usually not the primary concern when using logical constraint solvers.


\section{Optimization}
This section examines how well the second goal of the thesis, the learning of optimization criteria, has been accomplished.
Contrary to the previous section, the evaluation will focus only on the accuracy with which the learned soft constraints are able to approximate an underlying model.
Different factors will be explored that influence the results of the clausal optimization system.

\subsection{Setup}

\paragraph{Housing}
The primary example for clausal optimization revolves around housing.
There are three areas ($A_1$, $A_2$ and $A_3$) which have certain characteristics such as whether the area has a low crime rate and is cheap to live in.
In this example the user is trying to decide 1) where to live, 2) where to work and 3) which school to select.
Any area can be chosen for living and all areas have a school.
The areas to work in are restricted by the job offers that have been received.

\begin{table}[!htp]
	\begin{tabularx}{\textwidth}{l|r||*3{>{\centering\arraybackslash}X}}
	    & \textbf{Category} & \textbf{Area 1} & \textbf{Area 2} & \textbf{Area 3} \\
	    \midrule
	    \textbf{Properties} & Low crime & x & x & \\
	    & Cheap & x & & x \\
	    \midrule
	    \textbf{Possible} & House & x & x & x \\
	    \textbf{locations} & Job offer & x & x & \\
	    & School & x & x & x 
	\end{tabularx}
	\label{tbl:setup_housing}
	\caption{Characteristics and restrictions}
\end{table}

The different possible choices form $18$ distinct examples.
Using variables $x, y$ and~$z$ to denote the selected living, work and school areas, respectively, a template example can be formulated as:

\begin{figure}[!htp]
	\begin{minipage}{0.5\textwidth}
		\begin{verbatim*}
			const Area A1 A2
			const Area A3 A4

			low_crime(A1)
			low_crime(A2)
			low_crime(A4)
		\end{verbatim*}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\begin{verbatim*}
			cheap(A1)
			cheap(A3)

			live_in(x)
			work_in(z)
			school_in(y)
		\end{verbatim*}
	\end{minipage}
	\label{fig:setup_housing_example_template}
	\caption{Housing example template}
\end{figure}

In order to emulate real user preferences, a model is used used to generate pair wise rankings over the examples.
The standard model consists of $4$ weighted constraints:
\begin{shiftedflalign*}
	& \text{ }0.50 : \mathit{low\_crime}(a) \leftarrow \mathit{live\_in}(a) & \\
	& \text{ }0.25 : \mathit{school\_in}(a) \leftarrow \mathit{work\_in}(a) & \\
	& \text{ }1.00 : \mathit{low\_crime}(a) \leftarrow \mathit{school\_in}(a) & \\
	& \text{-}1.00 : \mathit{false} \leftarrow \mathit{live\_in}(a) \land \mathit{cheap}(a) &
\end{shiftedflalign*}

\subsection{Accuracy}

Generally, the experiments in this section follow one of two approaches.
\begin{description}
	\item[Approach 1] The available examples are split into two parts: the training and the test set.
	\item[Approach 2] All examples are used both as training and test set.
\end{description}
Both approaches then follow the same process, which is visualized in figure~\ref{fig:co_test_setup}.

First, all pairwise rankings over the examples in the training set are generated using the model constraints and a fraction of these are selected.
A fraction of these remaining rankings are then inverted to simulate noise.
The clausal optimization process is used to generate soft constraints which in turn are used generate pairwise ratings over the test set.
These pairwise rankings are compared to the pairwise rankings generated over the test set by the model constraints.
By counting the number of rankings that disagree and dividing this by the total number of rankings, one obtains the Kendall tau distance $\mathit{KD}$.
The score is calculated as $1 - \mathit{KD}$.

\begin{figure}

	\caption{Clausal optimization testing setup}
	\centering
		\includegraphics[width=1\textwidth]{COSetup.pdf}
	\label{fig:co_test_setup}

\end{figure}

Experiments are usually run $8$ times in a row, mainly to account for the randomness in selecting examples, rankings and noise.

\begin{question}
	How accurately can soft constraints learned through clausal optimization approximate the underlying model?
\end{question}

\begin{experiment}[\textsc{Accuracy approach 1}]

		"Some explanation?"
		\begin{figure}

		\caption{Effect of increasing fraction of rankings}
		\centering
			\includegraphics[width=.8\textwidth]{accuracy_splits_rankings}
		\label{fig:accuracy_splits_rankings}

	\end{figure}

	\begin{figure}

		\caption{Effect of increasing training set fraction}
		\centering
			\includegraphics[width=.8\textwidth]{accuracy_splits_training}
		\label{fig:accuracy_splits_training}

	\end{figure}

\end{experiment}

\begin{experiment}[\textsc{Accuracy approach 2}]

		"Write" 
		\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{XXX}
			\textbf{Fraction rankings}	& \textbf{Mean score}	& \textbf{StdDev} \\
			\toprule
			0.1 	& 0.855		& 0.050 \\
			0.2 	& 0.930		& 0.046 \\
			0.3 	& 0.980 	& 0.014 \\
		\end{tabularx}
		\label{tbl:exp_acc_approach2}
		\caption{Accuracy obtained using approach~2}
	\end{table}

\end{experiment}

\begin{experiment}[\textsc{Compare top score}]
	By solving the standard constraints, an optimal solution can be calculated:
	\begin{verbatim}
		live_in(A3), work_in(A1), school_in(A1)
	\end{verbatim}
	This solution fulfills constraints two and three and thus receives a score of $1.25$.
	Running clausal discovery with a training set and ranking fraction of $0.4$ resuls in a set of soft constraints.
	Solving for these constraints returns the optimal solution according to those constraints.
	\begin{verbatim}
		live_in(A3), work_in(A2), school_in(A2)
	\end{verbatim}
	The standard model can then be used to assign a score to this example and in this case the score is also $1.25$.
\end{experiment}

\begin{experiment}[\textsc{Underlying model}]
	The underlying model does not have to be generated using constraints.
	In fact, little assumptions can be made about the real preferences a user has.
	While clausal theories are theoretically very expressive, some important restrictions have been placed on it.
	By using a model that features disconnected clauses, the accuracy of the clausal optimization system can be tested for models it cannot exactly learn.
	Figure~\ref{fig:co_acc_model} shows the scores obtained on the alternative model consisting of:

	\begin{shiftedflalign*}
		& \text{ }0.50 : \mathit{false} \leftarrow \mathit{live\_in}(a_1) \land \textit{school\_in}(a_2) & \\
		& \text{ }0.50 : \mathit{false} \leftarrow \mathit{live\_in}(a_1) \land \textit{work\_in}(a_2) &
	\end{shiftedflalign*}

	\begin{figure}

		\caption{Comparison with disconnected clauses}
		\centering
			\includegraphics[width=.8\textwidth]{model}
		\label{fig:co_acc_model}

	\end{figure}


\end{experiment}

\paragraph{Discussion}
"Discuss"

\begin{question}
	What is the influence of noise on the learned optimization criteria?
\end{question}

\begin{experiment}[\textsc{Influence of noise}]
	In order to answer this question, this experiment primarily measures the influence of the amount of inverted rankings in the second approach.
	The housing example was run for varying fractions of rankings and varying fractions of noise.
	Figure~\ref{fig:nosplit_noise_noise} shows the effect of increasing noise on the score for several fractions of rankings.
	On figure~\ref{fig:nosplit_noise_rankings} the effect of increasing the amount of rankings can be seen, when the noise level is fixed.

	\begin{figure}

		\caption{Score for increasing noise}
		\centering
			\includegraphics[width=.8\textwidth]{nosplit_noise_noise}
		\label{fig:nosplit_noise_noise}

	\end{figure}

	\begin{figure}

		\caption{Effect of increasing fraction of rankings}
		\centering
			\includegraphics[width=.8\textwidth]{nosplit_noise_rankings}
		\label{fig:nosplit_noise_rankings}

	\end{figure}

\end{experiment}

\begin{experiment}[\textsc{Noise in approach 1}]
	
	The influence of noise is similar for approach~1.
	Figure~\ref{fig:hard_constraints} shows how the standard implementation (with hard constraints for pruning) performs for increasing noise levels.

\end{experiment}

\paragraph{Discussion}
"Discuss"

\begin{question}
	What is the effect of internal parameters or influences?
\end{question}

The experiments for this question explore the influence of several factors.
Unless mentioned otherwise, the experiments will use the following setup as baseline:
Approach~1, where the fraction of examples in the training set is chosen as $0.4$ and the fraction of rankings is also set to $0.4$.

\begin{experiment}
	
	Clausal optimization uses a cost factor which represents the cost of disagreeing with a provided ranking in favor of a simpler model (see section~\ref{sec:clausal_opt_approach}).
	This parameter can help avoid overfitting.
	The standard setting in the clausal optimization is $0.2$.
	In this experiment the influence of the cost factor has been measured for different noise levels.
	Figure~\ref{fig:c_factors} shows the influence of the cost factor on the score for fixed noise levels.

	\begin{figure}

		\caption{Influence of cost factor}
		\centering
			\includegraphics[width=.8\textwidth]{c_factors}
		\label{fig:c_factors}

	\end{figure}

\end{experiment}

\begin{experiment}[\textsc{Other kernel}]
	"Think about"
\end{experiment}

\begin{experiment}
	
	In the clause learning implementation the choice was made to allow hard constraints to prune soft constraints (see section~\ref{sec:clausal_discovery_approach}).
	This simplfies the model as fewer clauses are learned at the expense of pruning some clauses with discriminative power if the hard constraints just coincidently cover all the given examples.
	This experiment compares the standard implementation with an alternative implementation that does not allow hard constraints to prune for increasing noise levels.
	The results can be seen in figure~\ref{fig:hard_constraints}.

	\begin{figure}

		\caption{Pruning with and without hard constraint}
		\centering
			\includegraphics[width=.8\textwidth]{hard_constraints}
		\label{fig:hard_constraints}

	\end{figure}

\end{experiment}

\paragraph{Discussion}
"Discuss"