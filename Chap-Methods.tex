%!TEX root=Thesis.tex
\chapter{Approach}
\label{cha:meth}

[!! Bridge to previous chapter]

\begin{figure}

	\caption{High level structure}
	\centering
		\includegraphics[width=1\textwidth]{ApproachOverview.pdf}
	\label{fig:high_level_structure}

\end{figure}

Figure~\ref{fig:high_level_structure} provides an overview of the approach implemented in this thesis.
One feature of the system is clause learning, the ability to automatically learn clauses from examples.
At the core of the clause learning process is a new version of the clausal discovery system.
The main differences to clausal discovery engine Claudien (see section~\ref{sec:claudien}) are a different bias and a different refinement operator.
Furthermore, the clausal discovery program interacts with a generic logical system to calculate coverage and entailment.
The second feature is clausal optimization.
Hereby user-provided rankings and the \svm{} (see section~\ref{sec:svm}) program are used to produce weighted soft constraints.
These are clauses that have a weight assigned to them, expressing how desirable it is to fulfill that clause.

Clausal discovery and the clause learning / clausal optimization workflows are implemented in Java.
Interaction with the user, the logical system as well as the \svm{} software is achieved through files and standard input / output.

Through this chapter the map coloring problem will be used as an example for clause learning applications.
In this problem there is a set of countries and every country is assigned a color.
Some countries are neighbors of each other.
The main constraint is that neighboring countries may not have the same color.

\section{Output}
In order to understand the goal of this approach and this thesis, this section will provide more details on the produced outputs: constraints and weighted soft constraints.
For the application at hand, a problem can be viewed as a description of its domain and a set of rules.
The domain consists of all potential solutions to the problems and members of the problem domain will be referred to as instances or models.
If an instance fulfills all the rules, it is considered to be valid or a solution to the problem.
These rules can therefore be seen as \textit{hard} constraints, constraints which an instance must fulfill in order to form a solution to the problem.
Considering, for example, the map coloring problem, then all colored maps are potential solutions.
However, only maps in which countries never have the same color as their neighbors are actual solutions.

\begin{example}
Given multiple examples of correctly colored maps, the output of the clause learning system would consist of a set of clauses. One of the clauses would express the constraint: \cen{$false \leftarrow color(c_1, color_1) \land neighbor(c_1, c_2) \land color(c_2, color_1)$.}
Because of Object Identity (see chapter~\ref{cha:bg}), variables with different names always denote different object.
\end{example}

Learned constraints can be used to generate new solutions, check if an instance is valid or expand a partial instance into a solution.
The clauses produced by the system are considered hard constraints if they cover all the provided examples.
A user can, however, also provide a custom minimum support value, e.g. $80\%$, then all clauses are returned that cover at least $80\%$ of the provided examples.
This can be used to find constraints that hold most of the time but not always and is also a way to deal with noise on the examples.

Problems potentially have many solutions and not all solutions might be equally good.
Soft constraints are constraints which solutions might fulfill but do not have to.
By using soft constraints, one can discriminate between better and worse solutions.
Every soft constraint has a weight associated with it and every solution can be assigned a score by summing the weights of the soft constraints that it fulfills.

Clausal optimization uses clause learning to find soft constraints in examples.
It then automatically assign weights to them based on the preferences a user expresses over the examples.
For clausal optimization, negative weights are also allowed.
This means that examples that fulfill these negatively weighted constraints actually get scored less than the ones that do not.
One can obtain non-negative weights by negating the clauses that have negative weights.
The resulting logical formula would, however, not be a clause anymore.

The output of the clausal optimization system consists of set of weighted clauses: \cen{$weight_1: clause_1$\\...\\$weight_n: clause_n$.}
For a particular instance \sym{I}, a variable $v_i$ can be introduced for every clause $c_i$.
If $c_i$ is true for \sym{I}, then $v_i$ is $1$, otherwise it is $0$. The score of \sym{I} can then be calculated as:
\begin{equation}
\sum\limits^n w_i \cdot v_i
\end{equation}

As mentioned in section~\ref{sec:clausal_logic}, clauses are disjunctions or terms. A theory of clauses of consists of a conjunction of clauses. Therefore, it is a logical theory in CNF [!! Expand on why clauses.]

\section{Input}
The input to the clause learning system consists mainly of two parts: definitions and examples.
When learning optimization criteria a third type of information is required: partial rankings over the given examples.
Optionally a user can also provide logical background knowledge.
Background knowledge can be used to impose additional constraints on valid examples, explain how to generate predicates or specify already known facts.

\subsection{Definitions}
The definitions part of the input consists of general information that describes the problem domain and the search space.
This generic information consists mainly of typing information and predicate definitions and is independent of specific examples.

\subsubsection{Typing}
Every object in the problem domain has exactly one type.
Types are declared as:
\begin{shiftedflalign*}
& \textbf{type } Name &
\end{shiftedflalign*}
Using these declared types, objects in the domain can be partitioned into disjoint subsets.
Only using disjoint types limits the capability to correctly model more complicated problems.
Therefore, types are also allowed to have a parent-type.
If an object has type $t$, it belongs to type $t$ and any object that belongs to type $t$ also belongs to the parent-type $p$ of $t$.
Types that have a parent-type are declared as:
\begin{shiftedflalign*}
& \textbf{type } Name \textbf{ > } ParentType &
\end{shiftedflalign*}
Typing information relates directly to the problem domain and is usually easy to provide for the user.
It is valuable to the clause learning algorithm because it restricts the search space.
Because of these two characteristics, it was decided to include typing information as an input.

\begin{example}
	\label{ex:map_color_types}
	The map coloring problem has been described earlier.
	There are two types of objects that are important: countries and colors.
	Therefore, the following type declarations can be formulated:
	\begin{shiftedflalign*}
		& \textbf{type } Country & \\
		& \textbf{type } Color &
	\end{shiftedflalign*}
\end{example}
\subsubsection{Predicate definitions}
Predicates describe relations between objects in a domain.
In the definitions section a user defines the predicates that will be used to describe the relations that exist in the problem domain.
A predicate definition contains information about the name of the predicate, the number of arguments (the arity) and the type of every argument.
A standard predicate is declared as:
\begin{shiftedflalign*}
& \textbf{pred } Name(Type_1, ..., Type_n) &
\end{shiftedflalign*}

There are two variants of predicate definitions.
For a normal predicate like $\mathit{fatherOf}$ the terms $\mathit{fatherOf}(x, y)$ and $\mathit{fatherOf}(y, x)$ are not equal.
In some circumstances, however, predicates describe symmetric relationships.
Such predicates can be defined as:
\begin{shiftedflalign*}
& \textbf{symm } Name(Type, ..., Type) &
\end{shiftedflalign*}
Symmetric predicates can be used to describe predicates for which the order of the arguments does not matter, e.g. reciprocal friendships or membership of an object in an unordered set.
Since the position of an argument in a symmetric predicate is irrelevant, all arguments must be of the same type.
Defining symmetric predicates allows the user to provide background knowledge he has about a predicate directly to the clause generation algorithm.
Given this information, the system can avoid generating redundant clauses.

The second variant allows the user to specify predicates that are not explicitly provided, but rather are generated according to logical background knowledge provided by the user.
Such calculated predicates are expressed as:
\begin{shiftedflalign*}
& \textbf{calc } Name(Type_1, ..., Type_n) &
\end{shiftedflalign*}

Calculated predicates might be based on the values of other predicates.
In this case a user might want to generate clauses that contain the calculated predicate, but not all of the original predicates.
This is the function of the search directive: 
\begin{shiftedflalign*}
& \textbf{search } Predicate_1\  ...\  Predicate_m &
\end{shiftedflalign*}
Only the specified predicates will be included in the clause learning algorithm.
If no search directive is provided, all defined predicates are included.

\begin{example}
	\label{ex:map_color_predicates}
	In the map coloring problem, there are primarily two relations.
	The \textsc{color} relation specifies the color of a country and the (symmetric) \textsc{neighbor} relation that specifies that two countries are neighbors.
	Considering the types in example~\ref{ex:map_color_types}, the following predicate definitions can be used to express these relations:
	\begin{shiftedflalign*}
		& \textbf{pred } color(Country, Color) & \\
		& \textbf{symm } neighbor(Country, Country) &
	\end{shiftedflalign*}
\end{example}

\subsection{Examples}
While definitions are generic properties of the problem domain, examples describe specific instances or models that the user perceives as valid solutions to his problem.
Every example contains constant declarations that list the objects (constants) that are part of the model as well as their types.
Examples also contain an exhaustive list of the relations that hold on these objects.
\begin{shiftedflalign*}
& \textbf{example }Name\  \{ & \\
& \tabspace \textbf{const } Type\  Object_1\  ...\  Object_k & \\
& \tabspace Predicate(Object_1, ..., Object_n) & \\
& \} &
\end{shiftedflalign*}

\begin{example}
	To illustrate, consider the map coloring problem with the definitions given in example~\ref{ex:map_color_types} and~\ref{ex:map_color_predicates}.
	Consider the countries Belgium, the Netherlands and Luxembourg. Given the fact that Belgium shares a border with the Netherlands and Luxembourg, one example could be:
	\begin{shiftedflalign*}
		& \textbf{example }benelux\  \{ & \\
		& \tabspace \textbf{const } Country\  Belgium\  Luxembourg\  Netherlands & \\
		& \tabspace \textbf{const } Color\  Red\  Blue & \\\\
		& \tabspace neighbor(Belgium, Luxembourg) & \\
		& \tabspace neighbor(Belgium, Netherlands) & \\\\
		& \tabspace color(Belgium, Red) & \\
		& \tabspace color(Luxembourg, Blue) & \\
		& \tabspace color(Netherlands, Blue) & \\
		& \} &
	\end{shiftedflalign*}
	Currently symmetric predicates have to be fully specified, the implementation could, however, easily be changed to automate this.

\end{example}
[!! More details on format?]

\subsection{Logical background knowledge}
In many cases it can be useful to include background knowledge that the user already possesses.
This knowledge can be used to provide additional constraints that a user is aware of, thereby restricting what clauses are considered valid.
A user can also specify facts that he already knows and wants to exclude from the output.
Furthermore, it can be used to automatically calculate predicates.
Consider, for example, two predicates \textsc{crowded(Elevator)} and \textsc{in(Person, Elevator)}.
The examples provided by the user could contain information about which elevator a person is in and the background knowledge could be used to specify that an elevator is crowded if there are more than 8 people in it.

Logical background knowledge is specified in a file.
An important aspect of background knowledge is that it is passed straight to the logical system, as shown in figure~\ref{fig:high_level_structure}.
The format of the background knowledge is thus dependent on the logical system that is being used.

\subsection{Rankings}
Rankings are provided by a user and they express which solutions he prefers.
Given examples $e_1, ..., e_x$, rankings are declared as:
\begin{shiftedflalign*}
& \textbf{pref } e_1 = ... = e_i > ... > e_j = ... = e_x &
\end{shiftedflalign*}
This statement expresses that out of examples $1$ to $x$, examples $1$ to $i$ are considered the best and examples $j$ to $x$ the worst examples.
The clausal optimization system will use these rankings to assign weights to soft constraints.
Recall that a score can be calculated for a solution by summing the weights of the soft constraints that cover the solution.
The weights of the constraints are chosen in such a manner that the rankings induced by the score approximate the rankings provided by the user.


An alternative approach would be to use absolute scores for examples.
For a human, however, it is often hard to provide absolute scores over potentially many examples.
Rankings only require the user to consider few examples at a time and express his relative preferences over them.

\section{Clausal Discovery}
As stated previously, clausal discovery forms the core of the clause learning process.
Algorithm~\ref{alg:cd} in section~\ref{sec:claudien} describes the clausal discovery process at a high level.
The behavior of the algorithm is mainly defined by three functions: \textsc{covers(c, \sym{D})}, \textsc{entails(\sym{T}, c)} and \textsc{$\rho$(c)} - the refinement operator.

The algorithm keeps a queue of candidate clauses.
Whenever a clause is selected, it is tested for coverage.
If it succeeds, it is added to the result set, provided it is not entailed by the clauses already in the result set.
If it fails the coverage test, the refinement operator is used to produce generalizations of the clause, which are added to the result set.
Clauses are thus generalized until they pass the coverage test.

\begin{figure}[!htp]

	\caption{Coverage}
	\centering
		\includegraphics[width=0.7\textwidth]{CDCoverage.pdf}
	\label{fig:cd_coverage}

\end{figure}

\paragraph{Coverage} 
Coverage is calculated for each example separately and this calculation is outsourced to the logical system.
If an example violates a clause, the clause is too specific and does not cover the example.
Coverage tests occur frequently and form a significant amount of the total computation time.
These tests can, however, be parallelized to a certain degree, as there is a gap between the moment a clause is added to the queue and the moment it is selected.

\begin{figure}[!htp]

	\caption{Entailment}
	\centering
		\includegraphics[width=0.7\textwidth]{CDEntailment.pdf}
	\label{fig:cd_entailment}

\end{figure}


\paragraph{Entailment}
In order to avoid redundant clauses in the result set, a clause is only added to the result set if it is not logically entailed by the clauses in the result set.
This test is independent the specific examples and is also accomplished by the logical system.
Interaction with the logical system costs time and logical entailment is a hard problem.
Therefore, additional tests are run in the main program to discover some redundancies faster.

\paragraph{Refinement}
If a clause is too specific, it is rejected for not covering enough examples.
It is the task of the refinement operator to generate (minimal) generalizations of a clause, which are added back to the queue.
The refinement operator is thus responsible for exploring the search space.
It dictates what clauses are explored and in which order.
\\\\
In this section the different steps of the clausal discovery implementation will be explored.
The sequence of steps begins at the moment that a new clause is created by the refinement operator.

The algorithm explores the search space in a breadth-first fashion, using a first-in-first-out queue.
Clauses are generalized by the refinement operator by adding one term to the clause.
Therefore, the clauses are increasing in length over the course of the algorithm.
An important consequence of this process, is that shorter clauses are added to the result set first.
This implies, that the clause is maximally specific.

\subsection{Coverage calculation}
At the moment that a clause is created and to be added to the queue, the computation of the \textsc{covers} function is asynchronously dispatched.
By the time that a clause is selected from the queue, the results from the \textsc{covers} have, in the best case, already been calculated.

\subsection{Subset-test}
When a clause~$c$ is selected from the queue, a subset-test is performed.
The implementation keeps track of the set of clauses \sym{C} that have passed the coverage test, regardless of the result of the entailment test.
As mentioned above, clauses are processed in increasing length.
Assuming a complete refinement operator, any subset of $c$ that forms a valid clause on its own has already been generated.
Any clause has associated with it a set of examples covered by the clause~($c$).
If a subset clause~$c_s$ of~$c$ covers the same examples as~$c$ and $c_s \in \sym{C}$, then $c$ can be safely discarded.

\begin{example}
	Assume~\sym{C} is the set of clauses that have passed the coverage test so far. Consider clauses:
	\cen{$c_1 = \{\lnot p_1(x)\} \notin \sym{C}$\\
	$c_2 = \{\lnot p_2(x)\} \in \sym{C}$\\
	and $c = \{\lnot p_1(x), \lnot p_2(x)\}$.}
	Assume that $c_2$ covers the set of examples $\sym{E}_2$ and $c$ covers examples \sym{E}.
	Since $c_2 \subset c$, it always holds that $\sym{E}_2 \subseteq \sym{E}$.
	If $\sym{E}_2 = \sym{E}$, then the clause~$c$ is redundant because $c_2 \subset c$ is more specific than $c$ and covers the same examples.
	However, if $\sym{E}_2 \subset \sym{E}$, then $c$ covers more examples than~$c_2$ and should also be added to \sym{C}.
\end{example}

\subsection{Coverage-test}
For a clause $c$ to be added to the result set, it must cover a certain number examples.
When looking for hard constraints, all examples must be covered.
Alternatively, a minimum support threshold decides how many examples a clause has to cover.
If the clause covers at least the minimal required amount of examples, it will be submitted to the entailment-test.
If it does not cover enough examples, the refinement operator will be applied to it.

\subsection{Entailment-test}
The entailment tests filters out redundant clauses to obtain a minimal theory.
A clause $c$ is only added to the result set \sym{R} if it is not logically entailed by the clauses that have been added to the result set earlier.
By pruning newer clauses, the algorithm favors short clauses over long clauses.
Clauses are also pruned if they are entailed by the generated or provided background knowledge.
When learning soft constraints, the examples covered by a clause have to be considered.
Only the subset $\sym{R}_c \subset \sym{R}$ of clauses that cover the same examples as $c$ are allowed to prune $c$.
If $c$ is entailed by clauses that cover different examples, then it is still relevant.
\begin{example}
	Consider clauses: \cen{
		$c_1 = p_2(x) \leftarrow p_1(x)$ which covers example $e_1, e_2$ \\
		$c_2 = p_3(x) \leftarrow p_2(x)$ which covers example $e_3$ \\
		and $c = p_3(x) \leftarrow p_1(x)$ which covers example $e_2$.
	}
	Although clause~$c$ is entailed by clauses $c_1$ and~$c_2$, it covers a different example. Therefore, it should be added to the result set. Clause~$c$ could, for example, be a useful constraint for discriminating between $e_1$ and~$e_2$.
\end{example}

\subsection{Refinement operator}
[!! Rewrite]
In clausal discovery one starts with the empty clause $\square$ and in every step one atom is added to either the head or the body.
Given a clause \obj{c} the refinement operator $\rho$ returns a set $\rho(\obj{c})$ of child clauses whose length is $length(\obj{c}) + 1$.
What atoms are added to the clause depends on the current clause and the predicates that are used in the search.
The implementation on the refinement operator uses a specific order in which atoms are added to a clause to eliminate duplicate clauses.
Given a maximal amount of variables, a list of possible atoms is generated and every clause is represented as a sorted list of indices referring to elements in the list.
The remainder of this section will explain how to generate the list of atoms such that all possible clauses can at least be generated once.
\\\\
The number of arguments of a predicate is referred to as the predicates arity.
A predicate $\obj{p}_3$ of arity 3 takes 3 arguments, e.g. $\obj{p}_3(x, y, x)$.
One can observe that while the arity of the previous predicate was 3, the particular instance $\obj{p}_3(x, y, x)$ only features 2 different variables.
Such instance will be considered as having a rank of 2: $rank(\obj{p}_3(x, y, x)) = 2$.
Considering Object Identity, $\obj{p}_3(x, y, x)$ can be viewed as an blueprint of instances of predicate $\obj{p}_3$ that feature the first variable on the first and third place and the second, different, variable on the third place.
This blueprint or instance prototype represents the function $f^{\obj{p}_3}_{121}(v_1, v_2) = \obj{p}_3(v_1, v_2, v_1)$.
\\\\
Given the predicate $\obj{p}_3$ with arity 3, one can construct instance prototypes of rank 1 ($f^{p_3}_{111}$), rank 2 ($f^{p_3}_{112}$, $f^{p_3}_{121}$, $f^{p_3}_{211}$, $f^{p_3}_{122}$, $f^{p_3}_{212}$, $f^{p_3}_{221}$) and rank 3 ($f^{p_3}_{123}$).
The set $\sym{F}^r_{p_i}$ is then defined as the set of all possible instance prototypes $f^{p_i}_{x_1, ..., x_n}$ or $f^{p_i}_\mathbf{x}$ of rank $r$.
For every prototype it holds that every number in $[1, r]$ occurs at least once in $\mathbf{x}$ and $max(\mathbf{x}) = r$.
In clausal discovery the user provides a set \sym{P} of predicates that can occur in the generated clauses.
Given a set \sym{P}, the set $\sym{F}^r$ is defined as $\sym{F}^r = \bigcup \{\sym{F}^r_{p_i} | p_i \in \sym{P}\}$.
Such a set contains all instance prototypes of rank $r$.
The reader may recall that these prototypes are actually functions that map variables to a predicate instance.
Therefore, $\sym{F}^r$ can also be used as a function that takes $r$ variables and maps them to the set of predicate instances obtained by applying every function in $\sym{F}^r$ to those $r$ variables.
For example consider a set $\sym{F}^2 = \{f^{p_2}_{12}, f^{p_2}_{21}\}$ then $\sym{F}^2(x, y) = \{p_2(x, y), p_2(y, x)\}$.
\\\\
Suppose the maximal number of variables is 3 and the maximal arity of the given predicates is 2, then the atom list used for the clausal discovery algorithm is:
\begin{align*}
\sym{F}^1(v_1) \cup \sym{F}^2(v_1, v_2) \cup \sym{F}^2(v_1, v_3) \cup \sym{F}^1(v_2) \cup \sym{F}^2(v_2, v_3) \cup \sym{F}^1(v_3)
\end{align*}
Any clause with 3 variables can be represented as an ordered sequence of atoms drawn from this list.
\\\\
While clauses can be expressed as ordered sequences of indices in the atom list, not all sequences are valid clauses.
Different constraints limit the legal combination of atoms.
The first constraint is that clauses must be connected, which means that given a clause, the next atom must feature at least one variable that already exists in the clause.
Secondly variables are introduced in order, so after the introduction of $v_1$, the atom introducing $v_3$ cannot appear before the atom introducing $v_2$.
An atom can of course introduce multiple variables.
The third constraint is that atoms appearing in the head of the clause cannot introduce new variables.
Lastly the user can provide typing information which is used to exclude inconsistent atoms.
This can be done during the generation of the list by removing atoms which are inconsistent with the types of the predicate arguments (e.g. $f^{p_3}_{111}$ for $p_3(Type_1, Type_2, Type_1)$) and during the algorithm, if a newly added atom would be inconsistent with the types of the variables already introduced in the clause.

\section{Logical system} [!! Why IDP?]
\label{sec:logical_system}
One of the key features of the clause learning system implemented for this thesis, is the use of a logical system to compute two core functions: coverage and entailment.
The clausal discovery implementation makes use of a generic  interface - the LogicExecutor - to interact with a logical system.
An implementation of this interface provides the interaction with a specific system.
Currently, the clause learning implementation uses the IDP [!! Ref] knowledge base system, which was described briefly in section~\ref{sec:logical_constraint_solving}.
IDP uses $FO(\cdotp)$, an extension of first order logic.
This extension supports types, aggregates, inductive definitions and more, which provides the possibility to provide rich background knowledge.
Unfortunately, the entailment functionality does not support all the features of this extension.
[!! Ref, generation vs test] IDP uses three elements to describe problems and constraints.
The first element is a vocabulary which contains type and predicate definitions.
Secondly, theories consisting of $FO(\cdotp)$ statements are used to express constraints.
The third element are structures, which represent partial or total instances or models.

\begin{figure}

	\caption{Conversion process}
	\centering
		\includegraphics[width=0.6\textwidth]{Coverage.pdf}
	\label{fig:conversion_to_logic}

\end{figure}

Figure~\ref{fig:conversion_to_logic} shows how to convert the different available information into the elements that IDP works with.
The vocabulary is generated from typing information and predicate definitions, while examples are converted to structures.
User provided background knowledge is already given in the form of a theory $\sym{B}_U$ and generated background knowledge - for symmetric predicates - is converted into a theory $\sym{B}_G$.
In IDP these two theories are then merged into a background theory \sym{B}.
Given the knowledge base, different types of inference can be run in IDP by defining procedures in the Lua scripting language.

\subsection{Coverage}
\textsc{covers(c, \sym{D})} calculates whether a clause $c$ covers the data \sym{D}.
In order to compute this function, a knowledge base and a procedure have to be synthesized.
Vocabulary, structures and background theory are generated as described above and the clause $c$ is converted to a logical theory $\sym{T}_c$.
For every example $e_i$ a structure $\sym{S}_i$ is generated.
The procedure in IDP attempts to expand each structure $\sym{S}_i$ with respect to $\sym{B} \cup \sym{T}_c$.
If such a model exists, $c$ is considered to cover $e_i$.
In practice the coverage for every example is calculated and the results are sent back to the clause learning algorithm.

\subsection{Entailment}
\textsc{entails(\sym{T}, c)} computes whether a theory \sym{T} entails the clause $c$.
The theory \sym{T} represents the theory formed by all clauses that have been found to cover the data so far.
It also contains the available background knowledge.d
IDP offers the capability to compute entailment between two theories.
This logical entailment is independent of any structures and uses a built-in SAT solver.
Aside from generating the vocabulary and background theories as usual, the theories $\sym{T}_R$ and $\sym{T}_c$ are calculated.
These represent the theory of clauses in the result set and of the given clause, respectively.
\sym{T} is then formed as $\sym{T} = \sym{B} \cup \sym{T}_R$ and using IDP it is calculated if \sym{T} entails $\sym{T}_c$.
The (boolean) result is returned to the clause learning system.
Using logical entailment identifies redundant clauses.
Not only does it identify clauses that are reformulations of each other, it also removes clauses which are a combination of other clauses.

\section{Clausal Optimization}
Clausal optimization takes two inputs: a set of (learned) clauses that each cover some, but not all of the provided examples as well as a set of rankings provided by the user.
Every ranking concerns a group of examples.
The goal of clausal optimization is to discriminate between examples based on soft constraints that hold on the examples and approximate the given rankings optimally.
To this end every example can be characterized as a set of boolean features.
Each of those features correspond with one of the clauses and indicates whether the example is covered by that clause.
Now, standard machine learning techniques can be used to learn weights for the features (clauses) based on the given rankings.
In this thesis \svm{} was chosen to accomplish this task.
[!! Why \svm{}?]

Weights are chosen optimally with respect to a cost parameter $c$, which determines the trade-off between disagreeing with a user provided ranking and a simpler model.
Assigning a low cost to disagreeing with the training rankings establishes a bias towards simpler models which can avoid over-fitting the training.
A high cost should be assigned, however, when one is interested in a model that is more accurate with respect to the given examples and rankings. [!! SVM rank margin]