%!TEX root=Thesis.tex
\chapter{Approach}
\label{cha:meth}

In this chapter, the clause learning and clausal discovery systems are outlined.
The clause learning system has the ability to automatically learn clauses from examples.
At the core of the clause learning process is a new version of the clausal discovery system.
The main differences to clausal discovery engine Claudien (see section~\ref{sec:claudien}) are a different bias and a different refinement operator.
Furthermore, the clausal discovery program interacts with a logical system (IDP) to calculate two main functions $\mathit{coverage}$ and \textit{entailment}.

The clausal optimization system uses user-provided rankings and learn-to-rank software (\svm) (see section~\ref{sec:svm}) to produce weighted soft constraints.
These soft constraints are represented as clauses and the clause learning system is used to find these clauses.

Figure~\ref{fig:high_level_structure} provides an overview of the approach implemented in this thesis.

\begin{figure}

	\caption{High level structure}
	\centering
		\includegraphics[width=1\textwidth]{ApproachOverview.pdf}
	\label{fig:high_level_structure}

\end{figure}

Both clause learning and clausal discovery are implemented in Java.
Interaction with the user, the logical system as well as the  software is achieved through files and standard input / output.

Throughout this chapter the map coloring problem will be used as an example for clause learning applications.
In this problem there is a set of countries and every country is assigned a color.
Some countries are neighbors of each other.
The main constraint is that neighboring countries may not have the same color.

\section{Output}
In order to understand the goal of this approach and this thesis, this section will provide more details on the produced outputs.
The aim of the clause learning system is to provide hard or soft constraints that describe valid solutions.
The clausal optimization system produces weighted soft constraints, that can be used as optimization criteria.

\paragraph{Constraints}
For the application at hand, a problem can be viewed as a description of its domain and a set of rules.
The domain consists of all potential solutions to the problems and members of the problem domain will be referred to as instances or models.
If an instance fulfills all the rules, it is considered to be valid or a solution to the problem.
These rules can therefore be seen as \textit{hard} constraints, constraints which an instance must fulfill in order to form a solution to the problem.
Considering, for example, the map coloring problem, then all colored maps are potential solutions.
However, only maps in which countries never have the same color as their neighbors are actual solutions.

In this thesis, constraints are represented as clauses and the clause learning process produces a set (or theory) of clauses.
Using clauses to represent constraints enables the use of learning algorithms from the inductive logic programming and the use of first order logic solvers and background knowledge.

\begin{example}
Given multiple examples of correctly colored maps, the output of the clause learning system would consist of a set of clauses. One of the clauses would express the constraint: \cen{$false \leftarrow color(c_1, color_1) \land neighbor(c_1, c_2) \land color(c_2, color_1)$.}
Because of Object Identity (see chapter~\ref{cha:bg}), variables with different names always denote different object.
\end{example}

Learned clauses can be used to generate new solutions, check if an instance is valid or expand a partial instance into a solution.
The clauses produced by the system are considered hard constraints if they cover all the provided examples.
A user can, however, also provide a custom minimum support value, e.g. $80\%$, then all clauses are returned that cover at least $80\%$ of the provided examples.
This can be used to find constraints that hold most of the time but not always and is also a way to deal with noise on the examples.

\paragraph{Optimization}
Problems potentially have many solutions and not all solutions might be equally good.
Soft constraints are constraints which solutions might fulfill but do not have to.
By using soft constraints, one can discriminate between better and worse solutions.
Every soft constraint has a weight associated with it and every solution can be assigned a score by summing the weights of the soft constraints that it fulfills.

Clausal optimization uses clause learning to find soft constraints in examples.
It then automatically assign weights to them based on the preferences a user expresses over the examples.
For clausal optimization, negative weights are also allowed.
This means that examples that fulfill these negatively weighted constraints actually get scored less than the ones that do not.
Using only positive weights would limit the expressiveness of the weighted clauses as optimization criteria.
One can obtain non-negative weights by negating the clauses that have negative weights.
The resulting logical formula would, however, not be a clause anymore.

The output of the clausal optimization system consists of set of weighted clauses: \cen{$weight_1: clause_1$\\...\\$weight_n: clause_n$.}
For a particular instance \sym{I}, a variable $v_i$ can be introduced for every clause $c_i$.
If $c_i$ is true for \sym{I}, then $v_i$ is $1$, otherwise it is $0$. The score of \sym{I} can then be calculated as:
\begin{equation}
\label{eq:weights_sum}
\sum\limits^n w_i \cdot v_i
\end{equation}

\section{Input}
The input to the clause learning system consists mainly of two parts: definitions and examples.
When learning optimization criteria a third type of information is required: partial rankings over the given examples.
Optionally a user can also provide logical background knowledge.
Background knowledge can be used to impose additional constraints on valid examples, explain how to generate predicates or specify already known facts.

\subsection{Definitions}
The definitions part of the input consists of general information that describes the problem domain and the search space.
This generic information consists mainly of typing information and predicate definitions and is independent of specific examples.

\subsubsection{Typing}
Every object in the problem domain has exactly one type.
Types are declared as:
\begin{shiftedflalign*}
& \textbf{type } Name &
\end{shiftedflalign*}
Using these declared types, objects in the domain can be partitioned into disjoint subsets.
Only using disjoint types limits the capability to correctly model more complicated problems.
Therefore, types are also allowed to have a parent-type.
If an object has type $t$, it belongs to type $t$ and any object that belongs to type $t$ also belongs to the parent-type $p$ of $t$.
Types that have a parent-type are declared as:
\begin{shiftedflalign*}
& \textbf{type } Name \textbf{ > } ParentType &
\end{shiftedflalign*}
Typing information relates directly to the problem domain and is usually easy to provide for the user.
It is valuable to the clause learning algorithm because it restricts the search space.
Because of these two characteristics, it was decided to include typing information as an input.

\begin{example}
	\label{ex:map_color_types}
	The map coloring problem has been described earlier.
	There are two types of objects that are important: countries and colors.
	Therefore, the following type declarations can be formulated:
	\begin{shiftedflalign*}
		& \textbf{type } Country & \\
		& \textbf{type } Color &
	\end{shiftedflalign*}
\end{example}
\subsubsection{Predicate definitions}
Predicates describe relations between objects in a domain.
In the definitions section a user defines the predicates that will be used to describe the relations that exist in the problem domain.
A predicate definition contains information about the name of the predicate, the number of arguments (the arity) and the type of every argument.
A standard predicate is declared as:
\begin{shiftedflalign*}
& \textbf{pred } Name(Type_1, ..., Type_n) &
\end{shiftedflalign*}

There are two variants of predicate definitions.
For a normal predicate like $\mathit{fatherOf}$ the atoms $\mathit{fatherOf}(x, y)$ and $\mathit{fatherOf}(y, x)$ are not equal.
In some circumstances, however, predicates describe symmetric relationships.
Such predicates can be defined as:
\begin{shiftedflalign*}
& \textbf{symm } Name(Type, ..., Type) &
\end{shiftedflalign*}
Symmetric predicates can be used to describe predicates for which the order of the arguments does not matter, e.g. reciprocal friendships or membership of an object in an unordered set.
Since the position of an argument in a symmetric predicate is irrelevant, all arguments must be of the same type.
Defining symmetric predicates allows the user to provide background knowledge he has about a predicate directly to the clause generation algorithm.
Given this information, the system can avoid generating redundant clauses.

The second variant allows the user to specify predicates that are not explicitly provided, but rather are generated according to logical background knowledge provided by the user.
Such calculated predicates are expressed as:
\begin{shiftedflalign*}
& \textbf{calc } Name(Type_1, ..., Type_n) &
\end{shiftedflalign*}

Calculated predicates might be based on the values of other predicates.
In this case a user might want to generate clauses that contain the calculated predicate, but not all of the original predicates.
This is the function of the search directive: 
\begin{shiftedflalign*}
& \textbf{search } Predicate_1\  ...\  Predicate_m &
\end{shiftedflalign*}
Only the specified predicates will be included in the clause learning algorithm.
If no search directive is provided, all defined predicates are included.

\begin{example}
	\label{ex:map_color_predicates}
	In the map coloring problem, there are primarily two relations.
	The \textsc{color} relation specifies the color of a country and the (symmetric) \textsc{neighbor} relation that specifies that two countries are neighbors.
	Considering the types in example~\ref{ex:map_color_types}, the following predicate definitions can be used to express these relations:
	\begin{shiftedflalign*}
		& \textbf{pred } color(Country, Color) & \\
		& \textbf{symm } neighbor(Country, Country) &
	\end{shiftedflalign*}
\end{example}

\subsection{Examples}
While definitions are generic properties of the problem domain, examples describe specific instances or models that the user perceives as valid solutions to his problem.
Every example contains constant declarations that list the objects (constants) that are part of the model as well as their types.
Examples also contain an exhaustive list of the relations that hold on these objects.
\begin{shiftedflalign*}
& \textbf{example }Name\  \{ & \\
& \tabspace \textbf{const } Type\  Object_1\  ...\  Object_k & \\
& \tabspace Predicate(Object_1, ..., Object_n) & \\
& \} &
\end{shiftedflalign*}

\begin{example}
	To illustrate, consider the map coloring problem with the definitions given in example~\ref{ex:map_color_types} and~\ref{ex:map_color_predicates}.
	Consider the countries Belgium, the Netherlands and Luxembourg. Given the fact that Belgium shares a border with the Netherlands and Luxembourg, one example could be:
	\begin{shiftedflalign*}
		& \textbf{example }benelux\  \{ & \\
		& \tabspace \textbf{const } Country\  Belgium\  Luxembourg\  Netherlands & \\
		& \tabspace \textbf{const } Color\  Red\  Blue & \\\\
		& \tabspace neighbor(Belgium, Luxembourg) & \\
		& \tabspace neighbor(Belgium, Netherlands) & \\\\
		& \tabspace color(Belgium, Red) & \\
		& \tabspace color(Luxembourg, Blue) & \\
		& \tabspace color(Netherlands, Blue) & \\
		& \} &
	\end{shiftedflalign*}
	Currently symmetric predicates have to be fully specified, the implementation could, however, easily be changed to automate this.

\end{example}
[!! More details on format?]

\subsection{Logical background knowledge}
In many cases it can be useful to include background knowledge that the user already possesses.
This knowledge can be used to provide additional constraints that a user is aware of, thereby restricting what clauses are considered valid.
A user can also specify facts that he already knows and wants to exclude from the output.
Furthermore, it can be used to automatically calculate predicates.
Consider, for example, two predicates \textsc{crowded(Elevator)} and \textsc{in(Person, Elevator)}.
The examples provided by the user could contain information about which elevator a person is in and the background knowledge could be used to specify that an elevator is crowded if there are more than 8 people in it.

Logical background knowledge is specified in a file.
An important aspect of background knowledge is that it is passed straight to the logical system, as shown in figure~\ref{fig:high_level_structure}.
The format of the background knowledge is thus dependent on the logical system that is being used.

\subsection{Rankings}
Rankings are provided by a user and they express which solutions he prefers.
They are only used for clausal optimization and will be ignored during the clause learning step.
Given examples $e_1, ..., e_x$, rankings are declared as:
\begin{shiftedflalign*}
& \textbf{pref } e_1 = ... = e_i > ... > e_j = ... = e_x &
\end{shiftedflalign*}
This statement expresses that out of examples $1$ to $x$, examples $1$ to $i$ are considered the best and examples $j$ to $x$ the worst examples.
The clausal optimization system will use these rankings to assign weights to soft constraints.
Recall that a score can be calculated for a solution by summing the weights of the soft constraints that cover the solution.
The weights of the constraints are chosen in such a manner that the rankings induced by the score approximate the rankings provided by the user.


An alternative approach would be to use absolute scores for examples.
For a human, however, it is often hard to provide absolute scores over potentially many examples.
Rankings only require the user to consider few examples at a time and express his relative preferences over them.

\section{Clausal Discovery}
\label{sec:clausal_discovery_approach}
As stated previously, clausal discovery forms the core of the clause learning process.
Algorithm~\ref{alg:cd} (section~\ref{sec:claudien}, page~\pageref{alg:cd}) describes the clausal discovery process at a high level.
The behavior of the algorithm is mainly defined by three functions: \textsc{covers(c, \sym{D})}, \textsc{entails(\sym{T}, c)} and \textsc{$\rho$(c)} - the refinement operator.

The algorithm keeps a queue of candidate clauses.
Whenever a clause is selected, it is tested for coverage.
If it succeeds, it is added to the result set, provided it is not entailed by the clauses already in the result set.
If it fails the coverage test, the refinement operator is used to produce generalizations of the clause, which are added to the result set.
Clauses are thus generalized until they pass the coverage test.

\begin{figure}[!htp]

	\caption{Coverage}
	\centering
		\includegraphics[width=0.7\textwidth]{CDCoverage.pdf}
	\label{fig:cd_coverage}

\end{figure}

\paragraph{Coverage} 
Coverage is calculated for each example separately and this calculation is outsourced to the logical system.
If an example violates a clause, the clause is too specific and does not cover the example.
Coverage tests occur frequently and form a significant amount of the total computation time.
These tests can, however, be parallelized to a certain degree, as there is a gap between the moment a clause is added to the queue and the moment it is selected.

\begin{figure}[!htp]

	\caption{Entailment}
	\centering
		\includegraphics[width=0.7\textwidth]{CDEntailment.pdf}
	\label{fig:cd_entailment}

\end{figure}


\paragraph{Entailment}
In order to avoid redundant clauses in the result set, a clause is only added to the result set if it is not logically entailed by the clauses in the result set.
This test is independent the specific examples and is also accomplished by the logical system.
Interaction with the logical system costs time and logical entailment is a hard problem.
Therefore, additional tests are run in the main program to discover some redundancies faster.

\paragraph{Refinement}
If a clause is too specific, it is rejected for not covering enough examples.
It is the task of the refinement operator to generate (minimal) generalizations of a clause, which are added back to the queue.
The refinement operator is thus responsible for exploring the search space.
It dictates what clauses are explored and in which order.
\\\\
In this section the different steps of the clausal discovery implementation will be explored.
The sequence of steps begins at the moment that a new clause is created by the refinement operator.

The algorithm explores the search space in a breadth-first fashion, using a first-in-first-out queue.
Clauses are generalized by the refinement operator by adding one literal to the clause.
Therefore, the clauses are increasing in length over the course of the algorithm.
An important consequence of this process, is that shorter clauses are added to the result set first.
This implies, that the clause is maximally specific.

\subsection{Coverage calculation}
At the moment that a clause is created and to be added to the queue, the computation of the \textsc{covers} function is asynchronously dispatched.
By the time that a clause is selected from the queue, the results from the \textsc{covers} have, in the best case, already been calculated.

\subsection{Subset-test}
When a clause~$c$ is selected from the queue, a subset-test is performed.
The implementation keeps track of the set of clauses \sym{C} that have passed the coverage test, regardless of the result of the entailment test.
As mentioned above, clauses are processed in increasing length.
Assuming a complete refinement operator, any subset of $c$ that forms a valid clause on its own has already been generated.
Any clause has associated with it a set of examples covered by the clause~($c$).
If a subset clause~$c_s$ of~$c$ covers the same examples as~$c$ and $c_s \in \sym{C}$, then $c$ can be safely discarded.

\begin{example}
	Assume~\sym{C} is the set of clauses that have passed the coverage test so far. Consider clauses:
	\cen{$c_1 = \{\lnot p_1(x)\} \notin \sym{C}$\\
	$c_2 = \{\lnot p_2(x)\} \in \sym{C}$\\
	and $c = \{\lnot p_1(x), \lnot p_2(x)\}$.}
	Assume that $c_2$ covers the set of examples $\sym{E}_2$ and $c$ covers examples \sym{E}.
	Since $c_2 \subset c$, it always holds that $\sym{E}_2 \subseteq \sym{E}$.
	If $\sym{E}_2 = \sym{E}$, then the clause~$c$ is redundant because $c_2 \subset c$ is more specific than $c$ and covers the same examples.
	However, if $\sym{E}_2 \subset \sym{E}$, then $c$ covers more examples than~$c_2$ and should also be added to \sym{C}.
\end{example}

\subsection{Coverage-test}
For a clause $c$ to be added to the result set, it must cover a certain number examples.
When looking for hard constraints, all examples must be covered.
Alternatively, a minimum support threshold decides how many examples a clause has to cover.
If the clause covers at least the minimal required amount of examples, it will be submitted to the entailment-test.
If it does not cover enough examples, the refinement operator will be applied to it.

\subsection{Entailment-test}
The entailment tests filters out redundant clauses to obtain a minimal theory.
A clause $c$ is only added to the result set \sym{R} if it is not logically entailed by the clauses that have been added to the result set earlier.
By pruning newer clauses, the algorithm favors short clauses over long clauses.
Clauses are also pruned if they are entailed by the generated or provided background knowledge.
When learning soft constraints, the examples covered by a clause have to be considered.
Only the subset $\sym{R}_c \subset \sym{R}$ of clauses that cover the same examples as $c$ are allowed to prune $c$.
If $c$ is entailed by clauses that cover different examples, then it is still relevant.
\begin{example}
	Consider three examples $e_1, e_2$ and~$e_3$ and clauses:\\

	\begin{tabular}{rl|llll}
		& Clause & Case 1 & Case 2 & Case 3 & Case 4 \\
		\midrule
		$c_1$ & $p_2(x) \leftarrow p_1(x)$ & $e_1$ & $e_1$ & $e_1, e_2$ & $e_1, e_2, e_3$\\
		$c_2$ & $p_3(x) \leftarrow p_2(x)$ & $e_1$ & $e_2$ & $e_1, e_3$ & $e_1$ \\
		$c$ & $p_3(x) \leftarrow p_1(x)$ & $e_1$ & $e_1$ & $e_1$ & $e_1$
	\end{tabular}
	\\\\
	where every case shows the examples covered by each clause.
	Clauses $c_1$ and~$c_2$ logically entail clause~$c$.

	\begin{description}
		\item[Case 1]
			All clauses cover the same examples and $c$ will not be added to the result set because it is redundant.
		\item[Case 2]
			Clause~$c_2$ covers a different example than~$c$.
			Therefore, it cannot be used to prune~$c$ and~$c$.
		\item[Case 3]
			Here, clauses $c_1$ and~$c_2$ both cover a superset of the examples covered by~$c$.
			While clause~$c$ is redundant, it can be used to discriminate $e_1$ from the other examples.
			For example, in the clausal optimization setting, such clauses are useful.
			In order to preserve such clauses, $\sym{R}_c$ does not include soft constraints that cover a superset of the examples that~$c$ covers.
		\item[Case 4]
			This case is related to case~3.
			However, clause~$c_1$ covers all examples.
			Therefore, it is considered to be a hard constraint, like the background knowledge.
			Including these constraints in $\sym{R}_c$ yields a more compact result set at the risk of excluding some clauses if the clauses only covers all examples by chance.
	\end{description}

\end{example}

\subsection{Refinement operator}
In clausal discovery one starts with the empty clause $\square$ and in every step one atom is added to either the head or the body.
Given a clause $c$, the refinement operator $\rho$ returns a set $\rho(\obj{c})$ of child clauses whose length is $length(\obj{c}) + 1$.
What atoms are added to the clause depends on the current clause and the predicates that are used in the search.
The implementation of the refinement operator uses a specific order in which atoms are added to a clause to eliminate duplicate clauses.
Given a maximal amount of variables, a list of possible atoms is generated and every clause is represented as a sorted list of indices referring to elements in the list.
The following section will explain how to generate the list of atoms, such that all possible clauses are at least generated once.

\paragraph{Atom list generation}
The number of arguments of a predicate is referred to as the predicates arity.
A predicate $\obj{p}_3$ of arity 3 takes 3 arguments, e.g. $\obj{p}_3(x, y, x)$.
One can observe that, while the arity of the previous predicate was 3, it only features 2 different variables.
Such a predicate will be considered as having a rank of 2.
Considering Object Identity (see section~\ref{sec:bg_ref_op}), $\obj{p}_3(x, y, x)$ can be viewed as a blueprint for predicates of type $\obj{p}_3$ that feature the first variable on the first and third place and the second -  different - variable on the third place.
This blueprint or predicate prototype represents the function $f^{\obj{p}_3}_{121}(v_1, v_2) = \obj{p}_3(v_1, v_2, v_1)$.

\begin{example}
	The following examples illustrate the use of predicate prototypes:
	\begin{shiftedflalign*}
	f^{\obj{p}_3}_{111}(x) &= \obj{p}_3(x, x, x) \\
	f^{\obj{p}_3}_{121}(x, y) &= \obj{p}_3(x, y, x) \\
	f^{\obj{p}_3}_{123}(x, y, z) &= \obj{p}_3(x, y, z)
	\end{shiftedflalign*}
\end{example}

Given the predicate $\obj{p}_3$ with arity 3, one can construct predicate prototypes of rank 1, rank 2 and rank 3 (see example~\ref{ex:sets_of_prototypes}).
The set $\sym{F}^r_{p_i}$ is then defined as the set of all possible predicate prototypes $f^{p_i}_{x_1, ..., x_n}$ or $f^{p_i}_\mathbf{x}$ of rank $r$.
For every prototype it holds that every number in $[1, r]$ occurs at least once in $\mathbf{x}$ and $max(\mathbf{x}) = r$.

\begin{example}
	\label{ex:sets_of_prototypes}
	For predicate $p_3$, the following sets can be constructed:
	\begin{shiftedflalign*}
		\sym{F}^1_{p_3} &= \{f^{p_3}_{111}\} \\
		\sym{F}^2_{p_3} &= \{f^{p_3}_{112}, f^{p_3}_{121}, f^{p_3}_{211}, f^{p_3}_{122}, f^{p_3}_{212}, f^{p_3}_{221}\} \\
		\sym{F}^3_{p_3} &= \{f^{p_3}_{123}\}
	\end{shiftedflalign*}
\end{example}

In the clausal discovery setting, a user provides a set \sym{P} of predicates that can occur in the generated clauses.
Given a set \sym{P}, the set $\sym{F}^r$ is defined as $\sym{F}^r = \bigcup \{\sym{F}^r_{p_i} | p_i \in \sym{P}\}$.
Such a set contains all predicate prototypes of rank $r$.
The reader may recall that these prototypes are actually functions that map variables to a predicate.
Therefore, $\sym{F}^r$ can also be used as a function that takes $r$ variables and maps them to the set of predicates, obtained by applying every function in $\sym{F}^r$ to those $r$ variables.

\begin{example}
	Consider $\sym{P} = \{p_1, p_2\}$:
	\begin{shiftedflalign*}
		\sym{F}^1 &= \{f^{p_1}_{1}, f^{p_2}_{11}\} \\
		\sym{F}^1(x) &= \{p_1(x), p_2(x, x)\}
	\end{shiftedflalign*}
\end{example}

Finally, suppose the maximal number of variables is 3 and the maximal arity of the given predicates is 2, then the atom list for this problem is:
\begin{align*}
\sym{L} = \sym{F}^1(v_1) \cup \sym{F}^2(v_1, v_2) \cup \sym{F}^2(v_1, v_3) \cup \sym{F}^1(v_2) \cup \sym{F}^2(v_2, v_3) \cup \sym{F}^1(v_3)
\end{align*}
Any clause with 3 variables can be represented as an ordered sequence of atoms drawn from this list.
The list is sorted by variables, where $v_1 < v_2 < v_3$, and then predicates.
For example $\sym{F}^3(v_1, v_2, v_3)$ would occur before $\sym{F}^2(v_1, v_3)$.

\begin{example}
	\label{ex:ref_op_list}
	Consider, again, $\sym{P} = \{p_1, p_2\}$ and 2 variables, then:
	\begin{shiftedflalign*}
		\sym{L} &= \sym{F}^1(v_1) \cup \sym{F}^2(v_1, v_2) \cup \sym{F}^1(v_2) \\
		&= \{p_1(v_1), p_2(v_1, v_1), p_2(v_1, v_2), p_2(v_2, v_1), p_1(v_2), p_2(v_2, v_2)\}
	\end{shiftedflalign*}

	The clause $c = p_2(v_1, v_1) \leftarrow p_1(v_1) \land p_2(v_1, v_2)$ can be written as $\sym{L}(2) \leftarrow \sym{L}(1) \land \sym{L}(3)$ or $\{\lnot \sym{L}(1), \lnot \sym{L}(3), \sym{L}(2)\}$.
\end{example}

\paragraph{Building clauses}
The refinement operator uses the atom list to generalize clauses by adding atoms which occurs later in the atom list.
Clauses are build in two stages.
In the first stage, atoms are added to the body.
In the second stage, atoms are added to the head.
Before the second stage, the atom list is reset.
Atoms that have been added in the first stage cannot be added in the second stage (this would result in a redundant clause that is always true).

\begin{example}
	The clause shown in example~\ref{ex:ref_op_list} would be build as follows:
	\begin{shiftedflalign*}
		\textit{Stage 1 }  & \{\lnot \sym{L}(1)\} \\
		& \{\lnot \sym{L}(1), \lnot \sym{L}(3)\} \\
		\textit{Stage 2 }  & \{\lnot \sym{L}(1), \lnot \sym{L}(3), \sym{L}(2)\} = c
	\end{shiftedflalign*}

	The refinement operator would then generate the following clauses:
	\begin{shiftedflalign*}
		\rho(c) = \{
			&\{\lnot \sym{L}(1), \lnot \sym{L}(3), \sym{L}(2), \sym{L}(4)\}
				%(p_2(v_2, v_1) \lor p_2(v_1, v_1) \leftarrow p_1(v_1) \land p_2(v_1, v_2))
			\\
			&\{\lnot \sym{L}(1), \lnot \sym{L}(3), \sym{L}(2), \sym{L}(5)\}
				%(p_1(v_2) \lor p_2(v_1, v_1) \leftarrow p_1(v_1) \land p_2(v_1, v_2))
			\\
			&\{\lnot \sym{L}(1), \lnot \sym{L}(3), \sym{L}(2), \sym{L}(6)\}\}
				%(p_2(v_2, v_2) \lor p_2(v_1, v_1) \leftarrow p_1(v_1) \land p_2(v_1, v_2))
	\end{shiftedflalign*}
\end{example}

\paragraph{Syntactic restrictions}
Different restrictions limits what clauses are generated.
One such restriction is typing information, which is used statically and dynamically.
When the atom list is generated, atoms that are inconsistent with respect to the typing information of the predicates are removed from the list.
During the algorithm atoms that are inconsistent with the types that have been assigned to variables are not added.

\begin{example}
	Typing restrictions are illustrated on some examples using a type $T$ and two disjoint subtypes $S_1$ and $S_2$ of $T$.

	\begin{table}[!htp]
		\begin{tabularx}{\textwidth}{llX}
				\textbf{Type definition} & \textbf{Atom} & \textbf{Prune?} \\
				\toprule
				$p_2(S_1, S_2)$ & $p_2(v_1, v_1)$ & Statically \\
				$p_2(T, S_1)$ & $p_2(v_1, v_1)$ & Dynamically if $\mathit{type}(v_1) \neq S_1$ \\
				$p_1(T)$ & $p_1(v_1)$ & No (if there are no other types) 
		\end{tabularx}
	\end{table}

\end{example}

Clauses must also be connected and range restricted, which is checked at runtime.
To check if an atom~$a$ can be added to a clause~$c$, consider $V_a$, the set of variables occurring in~$a$ and $V_c$, the variables occurring in~$c$.
If $min(V_a) > max(V_c)$, then the atom is not added because the clause would not be connected.
Additionally, if more than one new variable is introduced in a new atom, they must be introduced in increasing order to avoid the generation of duplicate clauses.
If the clause is in stage 2 and $max(V_a) > max(V_c)$, then the atom is not added because the clause would not be range restricted.

This order of the atom list is important, because it assures that clauses are generated in a connected way by placing connecting atoms first.
For example, clauses containing only $v_2$ are generated after clauses that contain $v_1$ and~$v_2$.
Therefore, dropping the $k$ last literals always results in a valid, connected clause.

\paragraph{Representative clause}
Often a clause is not unique and there are multiple ways to formulate it.
Even though the use of the atom list removes most duplicates, there are still some clauses that are reformulations of other clauses.

\begin{example}
	Consider the graph coloring predicates $\mathit{neighbor}$ and $\mathit{color}$ and clauses:
	\begin{shiftedflalign*}
		c_1 &= \{\lnot \mathit{color}(v_1, v_2), \lnot \mathit{neighbor}(v_1, v_3)\} \\
		c_2 &= \{\lnot \mathit{neighbor}(v_1, v_2), \lnot \mathit{color}(v_1, v_3)\}
	\end{shiftedflalign*}
	Then clause~$c_2$ is just a reformulation of the clause~$c_1$.
\end{example}

\begin{example}
	A more complex example are the clauses:
	\begin{shiftedflalign*}
		c_3 &= \{\lnot \mathit{color}(v_1, v_2), \lnot \mathit{neighbor}(v_1, v_3), \lnot \mathit{color}(v_4, v_2)\} \\
		c_4 &= \{\lnot \mathit{color}(v_1, v_2), \lnot \mathit{color}(v_3, v_2), \lnot \mathit{neighbor}(v_3, v_4)\}
	\end{shiftedflalign*}
	These clauses again formulate the same constraint.
\end{example}

These duplicates are harder to detect, especially the second example.
To solve the problem of detecting these duplicates, this thesis suggests a procedure which will be referred to as ``representative test''.

The first step for detecting these duplicates is to define an order that specifies that, for example, clause~$c_3$ is ``smaller'' than clause~$c_4$.
This order can be provided by the atom list and looking at the positions of the smallest atom that does not occur in the other clause.

Using this order the representative clause can be defined as the smallest clause of all equivalent clauses.
If the current clause is the representative clause it is generated, otherwise is not.
The representative clause for a clause~$c$ is found by shuffling the literals in the clause.
If a shuffled clause can be built using the refinement operator that is smaller than the clause~$c$, $c$ can be discarded.

\begin{example}
	This example illustrates how the representative test dismisses clause~$c_4$.
	\begin{shiftedflalign*}
		c_4 = &\{\lnot \mathit{color}(v_1, v_2), \lnot \mathit{color}(v_3, v_2), \lnot \mathit{neighbor}(v_3, v_4)\} \\
		\mathit{shuffle} :\  &\{\lnot \mathit{color}(v_3, v_2), \lnot \mathit{neighbor}(v_3, v_4), \lnot \mathit{color}(v_1, v_2)\} \\
		\mathit{reorder\  variables} :\  &\{\lnot \mathit{color}(v_1, v_2), \lnot \mathit{neighbor}(v_1, v_3), \lnot \mathit{color}(v_4, v_2)\} = c_3 < c_4\\
	\end{shiftedflalign*}
\end{example}

There is only one representative clause for every clause and if a clause is not representative is it discarded.

\begin{proof}
	A representative clause must contain the same number of variables and the same predicates as the original clause.
	Since variables are introduced in a strict order, only the placement of the predicates can vary.
	There is exactly one minimal placement that results in a valid clause, as there must be at least one different literal.

	By generating permutations of literals in a clause, all possible placements of predicates are considered.
	Therefore, the representative clause must also be generated.
\end{proof}

% \paragraph{Optimality}
% In chapter~\ref{cha:bg} the notions of completeness and optimality for refinement operators are introduced.
% A refinement operator is complete if every clause is visited at least once and it is optimal if every clause is visited but once.

% The refinement operator suggested in this thesis is considered to be optimal, with respect to Object Identity and syntactic restrictions.

% \emph{(Completeness)}
% 	Any clause can be written as a set of literals from the atom list and therefore, it can be built incrementally.
% 	As shown previously, any clause can also be rewritten into a representative clause.
% 	The order of the atom list is chosen such that connecting literals are generated first.
% 	As a result, if the $k$ last literals of a clause $c$ are dropped, the resulting clause $c_k$ is also connected.
% 	A property of a representative clause is, that the clause 


% 	Leaving out the last $k$ literals $E_k$
% 	If the clause $r - e$ is generated, than $r$ will also be generated.
% 	But, since $r$ was representative, so must $r - e$.
% 	Otherwise a permutation of $r$ would be smaller than $r$ which is not possible since $r$ is representative.


% removing the last literal results in a valid clause, and if that clause is generated, then so will 


% Clauses that are 

\paragraph{Symmetric predicates}
For symmetric predicates the order of terms is irrelevant.
Internally, these predicates are always reduced to a unique form by ordering the variables that  occur in it.
Atoms that contains symmetric predicates with out-of-order variables can be deleted from the atom list.

\subsection{Pruning}
After the algorithm is finished, the resulting theory is pruned, because newer clauses could entail older clauses.
The procedure used for pruning is the same as for entailment.
For each clause an entailment test is run to see if the clause is entailed by the rest of the theory.
The resulting theory is more compact, without loosing expressiveness.

\section{Logical system}
\label{sec:logical_system}
One of the key features of the clause learning system implemented for this thesis, is the use of a logical system to compute two core functions: coverage and entailment.
The clausal discovery implementation makes use of a generic  interface - the LogicExecutor - to interact with a logical system.
An implementation of this interface provides the interaction with a specific system.

Currently, the clause learning implementation uses the IDP knowledge base system, which was described briefly in section~\ref{sec:logical_constraint_solving}.
IDP uses $FO(\cdotp)$, an extension of first order logic, which supports types, aggregates, inductive definitions and more.
Since the entailment functionality does not currently support all of these features, including these features in the background theory will require either providing a more limited theory for entailment than for coverage or suppressing the entailment test.

IDP uses three elements to describe problems and constraints.
The first element is a vocabulary which contains type and predicate definitions.
Secondly, theories consisting of $FO(\cdotp)$ statements are used to express constraints.
The third element are structures, which represent partial or total instances or models.

\begin{figure}

	\caption{Conversion process}
	\centering
		\includegraphics[width=0.6\textwidth]{Coverage.pdf}
	\label{fig:conversion_to_logic}

\end{figure}

Figure~\ref{fig:conversion_to_logic} shows how to convert the different available information into the elements that IDP works with.
The vocabulary is generated from typing information and predicate definitions, while examples are converted to structures.
User provided background knowledge is already given in the form of a theory $\sym{B}_U$ and generated background knowledge - for symmetric predicates - is converted into a theory $\sym{B}_G$.
In IDP these two theories are then merged into a background theory \sym{B}.
Given the knowledge base, different types of inference can be run in IDP by defining procedures in the Lua scripting language.

\subsection{Coverage}
\textsc{covers(c, \sym{D})} calculates whether a clause $c$ covers the data \sym{D}.
In order to compute this function, a knowledge base and a procedure have to be synthesized.
Vocabulary, structures and background theory are generated as described above and the clause $c$ is converted to a logical theory $\sym{T}_c$.
For every example $e_i$ a structure $\sym{S}_i$ is generated.
The procedure in IDP attempts to expand each structure $\sym{S}_i$ with respect to $\sym{B} \cup \sym{T}_c$.
If such a model exists, $c$ is considered to cover $e_i$.
In practice the coverage for every example is calculated and the results are sent back to the clause learning algorithm.

\subsection{Entailment}
\textsc{entails(\sym{T}, c)} computes whether a theory \sym{T} entails the clause $c$.
The theory \sym{T} represents the theory formed by all clauses that have been found to cover the data so far.
It also contains the available background knowledge.d
IDP offers the capability to compute entailment between two theories.
This logical entailment is independent of any structures and uses a built-in SAT solver.
Aside from generating the vocabulary and background theories as usual, the theories $\sym{T}_R$ and $\sym{T}_c$ are calculated.
These represent the theory of clauses in the result set and of the given clause, respectively.
\sym{T} is then formed as $\sym{T} = \sym{B} \cup \sym{T}_R$ and using IDP it is calculated if \sym{T} entails $\sym{T}_c$.
The (boolean) result is returned to the clause learning system.
Using logical entailment identifies redundant clauses.
Not only does it identify clauses that are reformulations of each other, it also removes clauses which are a combination of other clauses.

\section{Clausal Optimization}
\label{sec:clausal_opt_approach}
Clausal optimization needs two inputs: a set of (learned) clauses that each cover some, but not all of the provided examples as well as a set of rankings provided by the user.
Every ranking concerns a subset of the available examples.
The goal of clausal optimization is to discriminate between examples, based on soft constraints that hold on the examples, and approximate the given rankings optimally.

\paragraph{Soft constraints}
The soft constraints which are to be used as optimization criteria are learned using clause learning with a minimum threshold of 1 example.
Soft constraints with a low support can be interesting, for example, to distinguish between good (bad) solutions and the best (worst) solutions.
Since such constraints might also be due to over-fitting, the threshold could be increased if many examples are available.
Hard constraints are not used for ranking as they hold for all examples.

\paragraph{Learn to rank}
Examples are characterized by the soft constraints (clauses) which cover them.
Therefore, every example can be translated into a vector of boolean features.
Each feature $f_i$ corresponds with one of the clauses and is \emph{1} if the clause covers the example and \emph{0} otherwise.
Now existing machine learning software can be used to learn a linear function $\sum_i w_i \cdot f_i$ over these features, based on the given rankings.
For an unseen example, the feature vector is computed by calculating what clauses cover the new example.
That linear function can then be used to calculate a score for that example.

\begin{example}
	Consider examples~$e_1, e_2, e_3$, rankings $e_1 > e_2, e_2 > e_3$ and clauses $c_1, c_2, c_3$.
	If the clauses cover examples according to table~\ref{tbl:cover_examples}, then the function $(1 \cdot c_1) + (0\cdot c_2) + (-2\cdot c_3)$ perfectly models the given rankings.
	An example $e_4$ that is covered by~$c_1$ and~$c_2$, is then assigned a score of $1$.
	This score has no value in the absolute sense, it can only be used to compare it to other examples ranked by the same function.

	\begin{table}[!htp]
	\label{tbl:cover_examples}
	\begin{tabularx}{\textwidth}{c|ccc|X}
		\textbf{Clauses}	&$c_1$  	& $c_2$ 	& $c_3$ 	& \textbf{Feature vector}\\
		\toprule
		$e_1$ 				& Covers 	&  			&  			& 1, 0, 0\\
		$e_2$ 				& 			& Covers	&  			& 0, 1, 0\\
		$e_3$ 				& Covers 	& Covers 	& Covers 	& 1, 1, 1\\
	\end{tabularx}
	\caption{Clause coverage}
	\end{table}

\end{example}

In this thesis \svm{} was chosen to accomplish this task.
[!! "Why \svm{}?"]

\paragraph{Cost}
Weights are chosen optimally with respect to a cost parameter $c$, which determines the trade-off between disagreeing with a user provided ranking and a simpler model.
Assigning a low cost to disagreeing with the training rankings establishes a bias towards simpler models which can avoid over-fitting the training.
A high cost should be assigned, however, when one is interested in a model that is more accurate with respect to the given examples and rankings.

\section{Solving}
Using the clause learning and clausal optimization system enables a user to find formal representation of the constraints that describe the problem and user's preferences.
Clauses can be used in IDP to generate or check solutions to a problem by providing the learned constraints, defining a partial structure and performing model expansion on it.
It may be necessary to supply some additional information, such as that every country needs to be assigned a color.

\paragraph{MAX-SAT}
Weighted clauses can be obtained using the clausal optimization process.
Soft constraints found by the clause learning process can also be transformed into weighted clauses by using their frequency as a weight.
The problem of finding optimal solutions for these weighted clauses can be seen as a first order generalization of weighted MAX-SAT problem (see section~\ref{sec:max_sat}).

Solvers for weighted MAX-SAT attempt to optimize a formula consisting of a conjunction of positively weighted propositional clauses, by assigning values to the variables occurring in the formula.
In this thesis, first order logic clauses are used and the weights assigned to them can be positive or negative.
The optimization task is then very similar to the MAX-SAT setting:
Assign values to the variables such that the sum of the weights of the clauses that are satisfied is maximized.

\paragraph{Solving}
This optimization task can be solved directly in IDP, using inductive definitions, aggregates and minimization.
The only limitation is that the weights must be integer values.
Therefore, the weights of the clauses are divided by the smallest absolute weight, multiplied by a constant and rounded to the closest integer if necessary.
Removing this restriction for IDP is currently being researched and future releases might be able to deal with rational numbers directly.

In order to model the optimization problem in IDP, every clause $c_i$ with variables $v_1, ..., v_n$ is represented by a number $i$. For every clause a predicate $t(i)$ is added to capture the truth value of the clause.
A function $\mathit{cost}(i)$ specifies the cost of not satisfying the clause, which is equal to the integer weight of the clause.
\begin{shiftedflalign*}
	t(i) &\Leftrightarrow \forall v_1, ..., v_n : c_i. \\
	cost(i) &= w_1.
\end{shiftedflalign*}

Using $t$ and $\mathit{cost}$, a function $\mathit{actual}(i)$ is then defined in IDP as 0 if $t(i)$ and $\mathit{cost}(i)$ if $\lnot t(i)$.
This function is used in the term $\sum_i actual(i)$ to be minimized, which will allow IDP to search for an optimal solution.
The term can be expressed in IDP using the sum aggregate:
\begin{shiftedflalign*}
	sum\{i, cost : actual(i) = cost : cost\}
\end{shiftedflalign*}

The IDP code for a set of learned weighted clauses can easily be generated automatically.
An example is provided in appendix~\ref{app:co_idp}.